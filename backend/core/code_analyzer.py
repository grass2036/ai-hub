"""
ä»£ç åˆ†æå’ŒæŠ€æœ¯å€ºåŠ¡æ£€æµ‹æ¨¡å—
Week 6 Day 3: ä»£ç é‡æ„å’Œæ¶æ„ä¼˜åŒ– - ä»£ç åˆ†æ
å®ç°ä»£ç è´¨é‡åˆ†æã€æŠ€æœ¯å€ºåŠ¡æ£€æµ‹ã€é‡æ„å»ºè®®ç­‰åŠŸèƒ½
"""

import ast
import os
import re
import json
import logging
from typing import Dict, List, Any, Optional, Set, Tuple
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
import subprocess
import importlib.util

logger = logging.getLogger(__name__)

@dataclass
class CodeMetrics:
    """ä»£ç æŒ‡æ ‡"""
    file_path: str
    lines_of_code: int
    lines_of_comments: int
    complexity: int
    functions: int
    classes: int
    imports: int
    dependencies: List[str] = field(default_factory=list)
    issues: List[Dict[str, Any]] = field(default_factory=list)

@dataclass
class TechnicalDebt:
    """æŠ€æœ¯å€ºåŠ¡"""
    type: str
    severity: str  # low, medium, high, critical
    description: str
    file_path: str
    line_number: int
    suggestion: str
    estimated_hours: float

@dataclass
class RefactoringSuggestion:
    """é‡æ„å»ºè®®"""
    category: str
    priority: str
    description: str
    affected_files: List[str]
    benefits: List[str]
    effort_estimate: str

class CodeAnalyzer:
    """ä»£ç åˆ†æå™¨"""

    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.code_metrics: Dict[str, CodeMetrics] = {}
        self.technical_debts: List[TechnicalDebt] = []
        self.refactoring_suggestions: List[RefactoringSuggestion] = []

    def analyze_project(self) -> Dict[str, Any]:
        """åˆ†ææ•´ä¸ªé¡¹ç›®"""
        logger.info("å¼€å§‹ä»£ç ç»“æ„åˆ†æ...")

        # åˆ†æPythonæ–‡ä»¶
        python_files = self._find_python_files()
        for file_path in python_files:
            try:
                metrics = self._analyze_file(file_path)
                self.code_metrics[str(file_path)] = metrics
            except Exception as e:
                logger.error(f"åˆ†ææ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        # æ£€æµ‹æŠ€æœ¯å€ºåŠ¡
        self._detect_technical_debt()

        # ç”Ÿæˆé‡æ„å»ºè®®
        self._generate_refactoring_suggestions()

        return self._generate_analysis_report()

    def _find_python_files(self) -> List[Path]:
        """æŸ¥æ‰¾Pythonæ–‡ä»¶"""
        python_files = []
        for root, dirs, files in os.walk(self.project_root):
            # è·³è¿‡å¸¸è§çš„å¿½ç•¥ç›®å½•
            dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['__pycache__', 'venv', 'env', 'node_modules']]

            for file in files:
                if file.endswith('.py'):
                    python_files.append(Path(root) / file)

        return python_files

    def _analyze_file(self, file_path: Path) -> CodeMetrics:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # è§£æAST
            tree = ast.parse(content)

            # è®¡ç®—æŒ‡æ ‡
            lines = content.split('\n')
            loc = len(lines)
            comment_lines = len([line for line in lines if line.strip().startswith('#')])

            # è®¡ç®—å¤æ‚åº¦
            complexity = self._calculate_complexity(tree)

            # ç»Ÿè®¡å‡½æ•°å’Œç±»
            functions = len([node for node in ast.walk(tree) if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef))])
            classes = len([node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)])

            # ç»Ÿè®¡å¯¼å…¥
            imports = len([node for node in ast.walk(tree) if isinstance(node, (ast.Import, ast.ImportFrom))])

            # æå–ä¾èµ–
            dependencies = self._extract_dependencies(tree)

            # æ£€æµ‹é—®é¢˜
            issues = self._detect_code_issues(tree, content)

            return CodeMetrics(
                file_path=str(file_path),
                lines_of_code=loc,
                lines_of_comments=comment_lines,
                complexity=complexity,
                functions=functions,
                classes=classes,
                imports=imports,
                dependencies=dependencies,
                issues=issues
            )

        except Exception as e:
            logger.error(f"æ–‡ä»¶åˆ†æå¤±è´¥ {file_path}: {e}")
            return CodeMetrics(file_path=str(file_path), lines_of_code=0, lines_of_comments=0, complexity=0, functions=0, classes=0, imports=0)

    def _calculate_complexity(self, tree: ast.AST) -> int:
        """è®¡ç®—åœˆå¤æ‚åº¦"""
        complexity = 1  # åŸºç¡€å¤æ‚åº¦

        for node in ast.walk(tree):
            if isinstance(node, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1
            elif isinstance(node, ast.ExceptHandler):
                complexity += 1
            elif isinstance(node, ast.With, ast.AsyncWith):
                complexity += 1
            elif isinstance(node, ast.BoolOp):
                complexity += len(node.values) - 1

        return complexity

    def _extract_dependencies(self, tree: ast.AST) -> List[str]:
        """æå–ä¾èµ–"""
        dependencies = []

        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    dependencies.append(alias.name)
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    dependencies.append(node.module)

        return list(set(dependencies))

    def _detect_code_issues(self, tree: ast.AST, content: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹ä»£ç é—®é¢˜"""
        issues = []

        # æ£€æµ‹é•¿å‡½æ•°
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                func_lines = node.end_lineno - node.lineno if hasattr(node, 'end_lineno') else 0
                if func_lines > 50:
                    issues.append({
                        "type": "long_function",
                        "line": node.lineno,
                        "message": f"å‡½æ•° '{node.name}' è¿‡é•¿ ({func_lines} è¡Œ)",
                        "severity": "medium"
                    })

        # æ£€æµ‹å¤§ç±»
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                class_lines = node.end_lineno - node.lineno if hasattr(node, 'end_lineno') else 0
                if class_lines > 200:
                    issues.append({
                        "type": "large_class",
                        "line": node.lineno,
                        "message": f"ç±» '{node.name}' è¿‡å¤§ ({class_lines} è¡Œ)",
                        "severity": "medium"
                    })

        # æ£€æµ‹é‡å¤ä»£ç 
        lines = content.split('\n')
        repeated_lines = self._find_repeated_lines(lines)
        for line_info in repeated_lines:
            issues.append({
                "type": "duplicate_code",
                "line": line_info["line"],
                "message": f"ç–‘ä¼¼é‡å¤ä»£ç  (è¡Œ {line_info['line']})",
                "severity": "low"
            })

        return issues

    def _find_repeated_lines(self, lines: List[str], min_length: int = 5) -> List[Dict[str, int]]:
        """æŸ¥æ‰¾é‡å¤ä»£ç è¡Œ"""
        repeated_lines = []

        for i, line in enumerate(lines):
            if len(line.strip()) < min_length:
                continue

            # æŸ¥æ‰¾ç›¸ä¼¼è¡Œ
            for j in range(i + 1, len(lines)):
                other_line = lines[j]
                if len(other_line.strip()) < min_length:
                    continue

                # ç®€å•çš„ç›¸ä¼¼åº¦æ£€æµ‹
                similarity = self._calculate_similarity(line, other_line)
                if similarity > 0.8:
                    repeated_lines.append({
                        "line": i + 1,
                        "similar_line": j + 1,
                        "similarity": similarity
                    })
                    break

        return repeated_lines[:10]  # é™åˆ¶è¿”å›æ•°é‡

    def _calculate_similarity(self, str1: str, str2: str) -> float:
        """è®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦"""
        import difflib
        return difflib.SequenceMatcher(None, str1, str2).ratio()

    def _detect_technical_debt(self):
        """æ£€æµ‹æŠ€æœ¯å€ºåŠ¡"""
        self.technical_debts = []

        for file_path, metrics in self.code_metrics.items():
            # å¤æ‚åº¦è¿‡é«˜
            if metrics.complexity > 10:
                self.technical_debts.append(TechnicalDebt(
                    type="high_complexity",
                    severity="high" if metrics.complexity > 20 else "medium",
                    description=f"æ–‡ä»¶å¤æ‚åº¦è¿‡é«˜: {metrics.complexity}",
                    file_path=file_path,
                    line_number=1,
                    suggestion="é‡æ„å‡½æ•°ï¼Œå‡å°‘åµŒå¥—å’Œæ¡ä»¶åˆ†æ”¯",
                    estimated_hours=metrics.complexity * 0.5
                ))

            # æ³¨é‡Šç‡è¿‡ä½
            comment_ratio = metrics.lines_of_comments / metrics.lines_of_code if metrics.lines_of_code > 0 else 0
            if comment_ratio < 0.1 and metrics.lines_of_code > 50:
                self.technical_debts.append(TechnicalDebt(
                    type="low_documentation",
                    severity="medium",
                    description=f"æ³¨é‡Šä¸è¶³: {comment_ratio:.1%}",
                    file_path=file_path,
                    line_number=1,
                    suggestion="æ·»åŠ é€‚å½“çš„æ–‡æ¡£æ³¨é‡Š",
                    estimated_hours=2.0
                ))

            # ä¾èµ–è¿‡å¤š
            if len(metrics.dependencies) > 15:
                self.technical_debts.append(TechnicalDebt(
                    type="high_coupling",
                    severity="medium",
                    description=f"ä¾èµ–è¿‡å¤š: {len(metrics.dependencies)} ä¸ªæ¨¡å—",
                    file_path=file_path,
                    line_number=1,
                    suggestion="è€ƒè™‘ä½¿ç”¨ä¾èµ–æ³¨å…¥æˆ–æ¥å£æŠ½è±¡",
                    estimated_hours=4.0
                ))

    def _generate_refactoring_suggestions(self):
        """ç”Ÿæˆé‡æ„å»ºè®®"""
        self.refactoring_suggestions = []

        # åˆ†ææ¨¡å—åŒ–ç¨‹åº¦
        module_analysis = self._analyze_modularity()
        if module_analysis["needs_refactoring"]:
            self.refactoring_suggestions.append(RefactoringSuggestion(
                category="modularization",
                priority="high",
                description="æ”¹è¿›æ¨¡å—åŒ–è®¾è®¡ï¼Œå‡å°‘æ¨¡å—é—´è€¦åˆ",
                affected_files=module_analysis["affected_files"],
                benefits=["æé«˜ä»£ç å¯ç»´æŠ¤æ€§", "é™ä½æµ‹è¯•å¤æ‚åº¦", "å¢å¼ºä»£ç å¤ç”¨æ€§"],
                effort_estimate="2-3å¤©"
            ))

        # åˆ†æAPIè®¾è®¡
        api_analysis = self._analyze_api_design()
        if api_analysis["needs_refactoring"]:
            self.refactoring_suggestions.append(RefactoringSuggestion(
                category="api_design",
                priority="medium",
                description="ä¼˜åŒ–APIæ¥å£è®¾è®¡ï¼Œæé«˜ä¸€è‡´æ€§å’Œæ˜“ç”¨æ€§",
                affected_files=api_analysis["affected_files"],
                benefits=["æå‡å¼€å‘æ•ˆç‡", "å‡å°‘ç»´æŠ¤æˆæœ¬", "æ”¹å–„ç”¨æˆ·ä½“éªŒ"],
                effort_estimate="1-2å¤©"
            ))

        # åˆ†ææ•°æ®è®¿é—®å±‚
        data_analysis = self._analyze_data_access()
        if data_analysis["needs_refactoring"]:
            self.refactoring_suggestions.append(RefactoringSuggestion(
                category="data_access",
                priority="high",
                description="é‡æ„æ•°æ®è®¿é—®å±‚ï¼Œå¼•å…¥Repositoryæ¨¡å¼",
                affected_files=data_analysis["affected_files"],
                benefits=["æé«˜æ•°æ®è®¿é—®æ•ˆç‡", "å¢å¼ºæ•°æ®å®‰å…¨æ€§", "ç®€åŒ–æµ‹è¯•"],
                effort_estimate="2-4å¤©"
            ))

    def _analyze_modularity(self) -> Dict[str, Any]:
        """åˆ†ææ¨¡å—åŒ–ç¨‹åº¦"""
        # è®¡ç®—æ¨¡å—é—´ä¾èµ–
        dependency_graph = {}
        for file_path, metrics in self.code_metrics.items():
            dependency_graph[file_path] = metrics.dependencies

        # æ£€æµ‹å¾ªç¯ä¾èµ–
        circular_deps = self._detect_circular_dependencies(dependency_graph)

        # æ£€æµ‹é«˜è€¦åˆæ¨¡å—
        high_coupling_files = []
        for file_path, deps in dependency_graph.items():
            if len(deps) > 10:
                high_coupling_files.append(file_path)

        return {
            "needs_refactoring": len(circular_deps) > 0 or len(high_coupling_files) > 0,
            "circular_dependencies": circular_deps,
            "high_coupling_files": high_coupling_files,
            "affected_files": high_coupling_files + [dep[0] for dep in circular_deps]
        }

    def _detect_circular_dependencies(self, dependency_graph: Dict[str, List[str]]) -> List[Tuple[str, str]]:
        """æ£€æµ‹å¾ªç¯ä¾èµ–"""
        circular_deps = []
        visited = set()
        rec_stack = set()

        def dfs(node: str, path: List[str]):
            if node in rec_stack:
                # æ‰¾åˆ°å¾ªç¯
                cycle_start = path.index(node)
                cycle = path[cycle_start:] + [node]
                for i in range(len(cycle) - 1):
                    circular_deps.append((cycle[i], cycle[i + 1]))
                return

            if node in visited:
                return

            visited.add(node)
            rec_stack.add(node)

            if node in dependency_graph:
                for dep in dependency_graph[node]:
                    dfs(dep, path + [node])

            rec_stack.remove(node)

        for file_path in dependency_graph:
            if file_path not in visited:
                dfs(file_path, [])

        return list(set(circular_deps))

    def _analyze_api_design(self) -> Dict[str, Any]:
        """åˆ†æAPIè®¾è®¡"""
        api_files = []
        for file_path in self.code_metrics:
            if "api" in file_path.lower():
                api_files.append(file_path)

        # æ£€æŸ¥APIæ–‡ä»¶çš„é—®é¢˜
        issues = []
        for file_path in api_files:
            metrics = self.code_metrics[file_path]
            if metrics.functions > 20:
                issues.append(f"{file_path}: å‡½æ•°è¿‡å¤š ({metrics.functions})")
            if metrics.complexity > 15:
                issues.append(f"{file_path}: å¤æ‚åº¦è¿‡é«˜ ({metrics.complexity})")

        return {
            "needs_refactoring": len(issues) > 0,
            "issues": issues,
            "affected_files": api_files
        }

    def _analyze_data_access(self) -> Dict[str, Any]:
        """åˆ†ææ•°æ®è®¿é—®å±‚"""
        data_files = []
        for file_path in self.code_metrics:
            if any(keyword in file_path.lower() for keyword in ["model", "database", "db", "sql"]):
                data_files.append(file_path)

        # æ£€æŸ¥æ•°æ®è®¿é—®é—®é¢˜
        issues = []
        for file_path in data_files:
            metrics = self.code_metrics[file_path]
            # æ£€æŸ¥æ˜¯å¦æœ‰ç›´æ¥SQLæŸ¥è¯¢
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                if 'SELECT' in content.upper() or 'INSERT' in content.upper():
                    issues.append(f"{file_path}: åŒ…å«ç›´æ¥SQLæŸ¥è¯¢")

        return {
            "needs_refactoring": len(issues) > 0,
            "issues": issues,
            "affected_files": data_files
        }

    def _generate_analysis_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        total_files = len(self.code_metrics)
        total_loc = sum(m.lines_of_code for m in self.code_metrics.values())
        total_complexity = sum(m.complexity for m in self.code_metrics.values())
        avg_complexity = total_complexity / total_files if total_files > 0 else 0

        # æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡æŠ€æœ¯å€ºåŠ¡
        debt_by_severity = {}
        for debt in self.technical_debts:
            if debt.severity not in debt_by_severity:
                debt_by_severity[debt.severity] = 0
            debt_by_severity[debt.severity] += 1

        return {
            "summary": {
                "total_files": total_files,
                "total_lines_of_code": total_loc,
                "average_complexity": round(avg_complexity, 2),
                "total_functions": sum(m.functions for m in self.code_metrics.values()),
                "total_classes": sum(m.classes for m in self.code_metrics.values()),
                "technical_debt_count": len(self.technical_debts),
                "refactoring_suggestions": len(self.refactoring_suggestions)
            },
            "technical_debt": {
                "by_severity": debt_by_severity,
                "total_estimated_hours": sum(debt.estimated_hours for debt in self.technical_debts),
                "items": [
                    {
                        "type": debt.type,
                        "severity": debt.severity,
                        "description": debt.description,
                        "file_path": debt.file_path,
                        "suggestion": debt.suggestion,
                        "estimated_hours": debt.estimated_hours
                    }
                    for debt in sorted(self.technical_debts, key=lambda x: {"critical": 4, "high": 3, "medium": 2, "low": 1}[x.severity], reverse=True)
                ]
            },
            "refactoring_suggestions": [
                {
                    "category": suggestion.category,
                    "priority": suggestion.priority,
                    "description": suggestion.description,
                    "affected_files": suggestion.affected_files,
                    "benefits": suggestion.benefits,
                    "effort_estimate": suggestion.effort_estimate
                }
                for suggestion in self.refactoring_suggestions
            ],
            "code_quality": {
                "files": [
                    {
                        "path": metrics.file_path,
                        "loc": metrics.lines_of_code,
                        "complexity": metrics.complexity,
                        "functions": metrics.functions,
                        "classes": metrics.classes,
                        "issues_count": len(metrics.issues)
                    }
                    for metrics in sorted(self.code_metrics.values(), key=lambda x: x.complexity, reverse=True)
                ]
            },
            "generated_at": datetime.now().isoformat()
        }

# æµ‹è¯•å‡½æ•°
def test_code_analysis():
    """æµ‹è¯•ä»£ç åˆ†æåŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•ä»£ç åˆ†æåŠŸèƒ½...")

    analyzer = CodeAnalyzer("/Users/chiyingjie/code/git/ai-hub/backend")
    report = analyzer.analyze_project()

    print(f"\nğŸ“Š ä»£ç åˆ†ææŠ¥å‘Š:")
    print(f"æ€»æ–‡ä»¶æ•°: {report['summary']['total_files']}")
    print(f"æ€»ä»£ç è¡Œæ•°: {report['summary']['total_lines_of_code']}")
    print(f"å¹³å‡å¤æ‚åº¦: {report['summary']['average_complexity']}")
    print(f"æŠ€æœ¯å€ºåŠ¡é¡¹: {report['summary']['technical_debt_count']}")
    print(f"é‡æ„å»ºè®®: {report['summary']['refactoring_suggestions']}")

    if report['technical_debt']['items']:
        print(f"\nâš ï¸  ä¸»è¦æŠ€æœ¯å€ºåŠ¡:")
        for item in report['technical_debt']['items'][:5]:
            print(f"- {item['description']} ({item['severity']})")

    if report['refactoring_suggestions']:
        print(f"\nğŸ’¡ é‡æ„å»ºè®®:")
        for suggestion in report['refactoring_suggestions']:
            print(f"- {suggestion['description']} ({suggestion['priority']})")

if __name__ == "__main__":
    test_code_analysis()