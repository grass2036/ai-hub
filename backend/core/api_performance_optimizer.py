"""
APIæ€§èƒ½ä¼˜åŒ–æ¨¡å—
Week 6 Day 2: æ€§èƒ½ä¼˜åŒ–å’Œè°ƒä¼˜ - APIå“åº”æ—¶é—´å’Œç¼“å­˜æœºï¿½ï¿½ï¿½ä¼˜åŒ–
å®ç°APIç¼“å­˜ã€å“åº”ä¼˜åŒ–ã€è¿æ¥æ± ç®¡ç†ç­‰åŠŸèƒ½
"""

import asyncio
import time
import hashlib
import json
import logging
from typing import Dict, List, Any, Optional, Callable, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from functools import wraps, lru_cache
from contextlib import asynccontextmanager
import aiohttp
import aioredis
from fastapi import Request, Response, HTTPException
from fastapi.concurrency import run_in_threadpool
import psutil

from backend.config.settings import get_settings

settings = get_settings()
logger = logging.getLogger(__name__)

@dataclass
class APIMetrics:
    """APIæ€§èƒ½æŒ‡æ ‡"""
    endpoint: str
    method: str
    response_time: float
    status_code: int
    response_size: int
    timestamp: datetime
    user_id: Optional[str] = None
    cache_hit: bool = False

@dataclass
class CacheConfig:
    """ç¼“å­˜é…ç½®"""
    ttl: int = 300  # é»˜è®¤5åˆ†é’Ÿ
    max_size: int = 1000
    key_prefix: str = "api_cache"
    enabled: bool = True
    vary_on_user: bool = False
    vary_on_params: List[str] = field(default_factory=list)

@dataclass
class PerformanceThresholds:
    """æ€§èƒ½é˜ˆå€¼é…ç½®"""
    response_time_warning: float = 1.0  # ç§’
    response_time_critical: float = 3.0  # ç§’
    memory_usage_warning: float = 80.0  # ç™¾åˆ†æ¯”
    cpu_usage_warning: float = 80.0  # ç™¾åˆ†æ¯”
    error_rate_warning: float = 5.0  # ç™¾åˆ†æ¯”

class APICacheManager:
    """APIç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self):
        self.redis_client = None
        self.memory_cache: Dict[str, Dict[str, Any]] = {}
        self.cache_stats = {
            "hits": 0,
            "misses": 0,
            "sets": 0,
            "evictions": 0
        }
        self._setup_redis()

    async def _setup_redis(self):
        """è®¾ç½®Redisè¿æ¥"""
        try:
            self.redis_client = await aioredis.from_url(
                f"redis://{getattr(settings, 'redis_host', 'localhost')}:{getattr(settings, 'redis_port', 6379)}",
                encoding="utf-8",
                decode_responses=True
            )
            await self.redis_client.ping()
            logger.info("Redisè¿æ¥æˆåŠŸï¼Œå¯ç”¨åˆ†å¸ƒå¼ç¼“å­˜")
        except Exception as e:
            logger.warning(f"Redisè¿æ¥å¤±è´¥ï¼Œä½¿ç”¨å†…å­˜ç¼“å­˜: {e}")
            self.redis_client = None

    def _generate_cache_key(self, request: Request, config: CacheConfig) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # åŸºç¡€é”®
        key_parts = [config.key_prefix, request.method, request.url.path]

        # æ·»åŠ å‚æ•°
        if config.vary_on_params:
            query_params = dict(request.query_params)
            for param in config.vary_on_params:
                if param in query_params:
                    key_parts.append(f"{param}={query_params[param]}")

        # æ·»åŠ ç”¨æˆ·å·®å¼‚
        if config.vary_on_user:
            # ä»è¯·æ±‚ä¸­è·å–ç”¨æˆ·IDï¼ˆéœ€è¦æ ¹æ®å®é™…è®¤è¯æ–¹å¼è°ƒæ•´ï¼‰
            user_id = getattr(request.state, 'user_id', None)
            if user_id:
                key_parts.append(f"user={user_id}")

        # ç”Ÿæˆæœ€ç»ˆé”®
        key_string = ":".join(str(part) for part in key_parts)
        key_hash = hashlib.md5(key_string.encode()).hexdigest()
        return f"{config.key_prefix}:{key_hash}"

    async def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜"""
        # å°è¯•Redis
        if self.redis_client:
            try:
                cached_data = await self.redis_client.get(key)
                if cached_data:
                    self.cache_stats["hits"] += 1
                    return json.loads(cached_data)
            except Exception as e:
                logger.warning(f"Redisç¼“å­˜è¯»å–å¤±è´¥: {e}")

        # å›é€€åˆ°å†…å­˜ç¼“å­˜
        if key in self.memory_cache:
            cache_entry = self.memory_cache[key]
            if cache_entry["expires_at"] > datetime.now():
                self.cache_stats["hits"] += 1
                return cache_entry["data"]
            else:
                # è¿‡æœŸï¼Œåˆ é™¤
                del self.memory_cache[key]
                self.cache_stats["evictions"] += 1

        self.cache_stats["misses"] += 1
        return None

    async def set(self, key: str, value: Any, ttl: int = 300):
        """è®¾ç½®ç¼“å­˜"""
        # å°è¯•Redis
        if self.redis_client:
            try:
                await self.redis_client.setex(key, ttl, json.dumps(value, default=str))
                self.cache_stats["sets"] += 1
                return
            except Exception as e:
                logger.warning(f"Redisç¼“å­˜å†™å…¥å¤±è´¥: {e}")

        # å›é€€åˆ°å†…å­˜ç¼“å­˜
        expires_at = datetime.now() + timedelta(seconds=ttl)
        self.memory_cache[key] = {
            "data": value,
            "expires_at": expires_at
        }
        self.cache_stats["sets"] += 1

        # æ£€æŸ¥å†…å­˜ç¼“å­˜å¤§å°
        if len(self.memory_cache) > 1000:  # é™åˆ¶å†…å­˜ç¼“å­˜å¤§å°
            self._evict_oldest_entries()

    def _evict_oldest_entries(self):
        """æ¸…ç†æœ€æ—§çš„ç¼“å­˜æ¡ç›®"""
        # æŒ‰è¿‡æœŸæ—¶é—´æ’åºï¼Œåˆ é™¤æœ€æ—§çš„20%
        sorted_entries = sorted(
            self.memory_cache.items(),
            key=lambda x: x[1]["expires_at"]
        )

        evict_count = len(sorted_entries) // 5
        for i in range(evict_count):
            key = sorted_entries[i][0]
            del self.memory_cache[key]
            self.cache_stats["evictions"] += 1

    async def delete(self, key: str):
        """åˆ é™¤ç¼“å­˜"""
        if self.redis_client:
            try:
                await self.redis_client.delete(key)
            except Exception as e:
                logger.warning(f"Redisç¼“å­˜åˆ é™¤å¤±è´¥: {e}")

        if key in self.memory_cache:
            del self.memory_cache[key]

    async def clear_pattern(self, pattern: str):
        """æŒ‰æ¨¡å¼æ¸…ç†ç¼“å­˜"""
        if self.redis_client:
            try:
                keys = await self.redis_client.keys(pattern)
                if keys:
                    await self.redis_client.delete(*keys)
            except Exception as e:
                logger.warning(f"Redisæ¨¡å¼æ¸…ç†å¤±è´¥: {e}")

        # æ¸…ç†å†…å­˜ç¼“å­˜
        keys_to_delete = [k for k in self.memory_cache.keys() if pattern in k]
        for key in keys_to_delete:
            del self.memory_cache[key]
            self.cache_stats["evictions"] += 1

    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total_requests = self.cache_stats["hits"] + self.cache_stats["misses"]
        hit_rate = (self.cache_stats["hits"] / total_requests * 100) if total_requests > 0 else 0

        return {
            "cache_type": "redis" if self.redis_client else "memory",
            "hits": self.cache_stats["hits"],
            "misses": self.cache_stats["misses"],
            "hit_rate_percent": round(hit_rate, 2),
            "total_cached_items": len(self.memory_cache),
            "evictions": self.cache_stats["evictions"]
        }

class APIPerformanceOptimizer:
    """APIæ€§èƒ½ä¼˜åŒ–å™¨"""

    def __init__(self):
        self.cache_manager = APICacheManager()
        self.metrics: List[APIMetrics] = []
        self.thresholds = PerformanceThresholds()
        self.performance_stats = {
            "total_requests": 0,
            "avg_response_time": 0.0,
            "error_count": 0,
            "slow_requests": 0
        }

    @asynccontextmanager
    async def measure_request(self, request: Request):
        """æµ‹é‡è¯·æ±‚æ€§èƒ½çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss

        try:
            yield
        finally:
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss

            # è®°å½•æŒ‡æ ‡
            response_time = end_time - start_time
            memory_usage = (end_memory - start_memory) / 1024 / 1024  # MB

            # è¿™é‡Œéœ€è¦ä»å“åº”ä¸­è·å–çŠ¶æ€ç å’Œå¤§å°
            # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™äº›ä¼šä»middlewareä¸­è·å–
            status_code = getattr(request.state, 'status_code', 200)
            response_size = getattr(request.state, 'response_size', 0)

            metric = APIMetrics(
                endpoint=request.url.path,
                method=request.method,
                response_time=response_time,
                status_code=status_code,
                response_size=response_size,
                timestamp=datetime.now(),
                user_id=getattr(request.state, 'user_id', None),
                cache_hit=getattr(request.state, 'cache_hit', False)
            )

            self.metrics.append(metric)
            self._update_performance_stats(metric)

            # æ£€æŸ¥æ€§èƒ½é˜ˆå€¼
            await self._check_performance_thresholds(metric)

    def _update_performance_stats(self, metric: APIMetrics):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        self.performance_stats["total_requests"] += 1

        # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
        total = self.performance_stats["total_requests"]
        current_avg = self.performance_stats["avg_response_time"]
        new_avg = (current_avg * (total - 1) + metric.response_time) / total
        self.performance_stats["avg_response_time"] = new_avg

        # æ›´æ–°é”™è¯¯è®¡æ•°
        if metric.status_code >= 400:
            self.performance_stats["error_count"] += 1

        # æ›´æ–°æ…¢è¯·æ±‚è®¡æ•°
        if metric.response_time > self.thresholds.response_time_warning:
            self.performance_stats["slow_requests"] += 1

    async def _check_performance_thresholds(self, metric: APIMetrics):
        """æ£€æŸ¥æ€§èƒ½é˜ˆå€¼"""
        if metric.response_time > self.thresholds.response_time_critical:
            logger.critical(
                f"APIå“åº”æ—¶é—´è¿‡é•¿: {metric.endpoint} - {metric.response_time:.3f}s"
            )
        elif metric.response_time > self.thresholds.response_time_warning:
            logger.warning(
                f"APIå“åº”æ—¶é—´è¾ƒæ…¢: {metric.endpoint} - {metric.response_time:.3f}s"
            )

        # æ£€æŸ¥é”™è¯¯ç‡
        if self.performance_stats["total_requests"] > 100:
            error_rate = (
                self.performance_stats["error_count"] /
                self.performance_stats["total_requests"] * 100
            )
            if error_rate > self.thresholds.error_rate_warning:
                logger.warning(f"APIé”™è¯¯ç‡è¿‡é«˜: {error_rate:.2f}%")

    def cache_response(self, config: CacheConfig):
        """APIå“åº”ç¼“å­˜è£…é¥°å™¨"""
        def decorator(func: Callable):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # å°è¯•ä»requestå‚æ•°è·å–è¯·æ±‚å¯¹è±¡
                request = None
                for arg in args:
                    if isinstance(arg, Request):
                        request = arg
                        break

                if not request or not config.enabled:
                    return await func(*args, **kwargs)

                # ç”Ÿæˆç¼“å­˜é”®
                cache_key = self.cache_manager._generate_cache_key(request, config)

                # å°è¯•è·å–ç¼“å­˜
                cached_response = await self.cache_manager.get(cache_key)
                if cached_response:
                    request.state.cache_hit = True
                    return cached_response

                # æ‰§è¡ŒåŸå‡½æ•°
                response = await func(*args, **kwargs)

                # ç¼“å­˜å“åº”ï¼ˆåªç¼“å­˜æˆåŠŸå“åº”ï¼‰
                if hasattr(response, 'status_code') and response.status_code == 200:
                    await self.cache_manager.set(cache_key, response, config.ttl)

                return response

            return wrapper
        return decorator

    async def invalidate_cache_pattern(self, pattern: str):
        """æŒ‰æ¨¡å¼å¤±æ•ˆç¼“å­˜"""
        await self.cache_manager.clear_pattern(f"{self.cache_manager.cache_stats['key_prefix']}:{pattern}")

    def get_performance_report(self, minutes: int = 60) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        cutoff_time = datetime.now() - timedelta(minutes=minutes)
        recent_metrics = [m for m in self.metrics if m.timestamp > cutoff_time]

        if not recent_metrics:
            return {"status": "no_data", "message": "æš‚æ— æ€§èƒ½æ•°æ®"}

        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        response_times = [m.response_time for m in recent_metrics]
        status_codes = [m.status_code for m in recent_metrics]

        # æŒ‰ç«¯ç‚¹åˆ†ç»„ç»Ÿè®¡
        endpoint_stats = {}
        for metric in recent_metrics:
            endpoint = metric.endpoint
            if endpoint not in endpoint_stats:
                endpoint_stats[endpoint] = {
                    "count": 0,
                    "response_times": [],
                    "status_codes": [],
                    "cache_hits": 0
                }

            endpoint_stats[endpoint]["count"] += 1
            endpoint_stats[endpoint]["response_times"].append(metric.response_time)
            endpoint_stats[endpoint]["status_codes"].append(metric.status_code)
            if metric.cache_hit:
                endpoint_stats[endpoint]["cache_hits"] += 1

        # è®¡ç®—æ¯ä¸ªç«¯ç‚¹çš„ç»Ÿè®¡
        for endpoint, stats in endpoint_stats.items():
            times = stats["response_times"]
            stats["avg_response_time"] = sum(times) / len(times)
            stats["max_response_time"] = max(times)
            stats["min_response_time"] = min(times)
            stats["cache_hit_rate"] = (
                stats["cache_hits"] / stats["count"] * 100
            )

        return {
            "summary": {
                "total_requests": len(recent_metrics),
                "avg_response_time": sum(response_times) / len(response_times),
                "max_response_time": max(response_times),
                "min_response_time": min(response_times),
                "error_rate": (
                    len([s for s in status_codes if s >= 400]) / len(status_codes) * 100
                ),
                "period_minutes": minutes
            },
            "endpoint_stats": endpoint_stats,
            "cache_stats": self.cache_manager.get_cache_stats(),
            "slow_requests": [
                {
                    "endpoint": m.endpoint,
                    "method": m.method,
                    "response_time": m.response_time,
                    "timestamp": m.timestamp.isoformat()
                }
                for m in sorted(recent_metrics, key=lambda x: x.response_time, reverse=True)[:10]
                if m.response_time > self.thresholds.response_time_warning
            ],
            "performance_thresholds": {
                "response_time_warning": self.thresholds.response_time_warning,
                "response_time_critical": self.thresholds.response_time_critical
            }
        }

    def clear_old_metrics(self, hours: int = 24):
        """æ¸…ç†æ—§çš„æ€§èƒ½æŒ‡æ ‡"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        original_count = len(self.metrics)

        self.metrics = [m for m in self.metrics if m.timestamp > cutoff_time]
        cleaned_count = original_count - len(self.metrics)

        if cleaned_count > 0:
            logger.info(f"æ¸…ç†äº† {cleaned_count} æ¡æ—§çš„APIæ€§èƒ½æŒ‡æ ‡")

class ConnectionPoolOptimizer:
    """è¿æ¥æ± ä¼˜åŒ–å™¨"""

    def __init__(self):
        self.aiohttp_session: Optional[aiohttp.ClientSession] = None

    async def get_optimized_session(self) -> aiohttp.ClientSession:
        """è·å–ä¼˜åŒ–çš„HTTPä¼šè¯"""
        if self.aiohttp_session is None or self.aiohttp_session.closed:
            connector = aiohttp.TCPConnector(
                limit=100,  # æ€»è¿æ¥æ± å¤§å°
                limit_per_host=30,  # æ¯ä¸ªä¸»æœºçš„è¿æ¥æ•°
                ttl_dns_cache=300,  # DNSç¼“å­˜æ—¶é—´
                use_dns_cache=True,
                keepalive_timeout=60,  # ä¿æŒè¿æ¥æ—¶é—´
                enable_cleanup_closed=True
            )

            timeout = aiohttp.ClientTimeout(
                total=30,  # æ€»è¶…æ—¶æ—¶é—´
                connect=10,  # è¿æ¥è¶…æ—¶
                sock_read=10  # è¯»å–è¶…æ—¶
            )

            self.aiohttp_session = aiohttp.ClientSession(
                connector=connector,
                timeout=timeout
            )

        return self.aiohttp_session

    async def close(self):
        """å…³é—­è¿æ¥æ± """
        if self.aiohttp_session and not self.aiohttp_session.closed:
            await self.aiohttp_session.close()

# å…¨å±€å®ä¾‹
api_performance_optimizer = APIPerformanceOptimizer()
connection_pool_optimizer = ConnectionPoolOptimizer()

# è£…é¥°å™¨å·¥å‚å‡½æ•°
def cache_api_response(
    ttl: int = 300,
    max_size: int = 1000,
    vary_on_user: bool = False,
    vary_on_params: List[str] = None
):
    """APIå“åº”ç¼“å­˜è£…é¥°å™¨"""
    config = CacheConfig(
        ttl=ttl,
        max_size=max_size,
        vary_on_user=vary_on_user,
        vary_on_params=vary_on_params or []
    )
    return api_performance_optimizer.cache_response(config)

# æ€§èƒ½ç›‘æ§è£…é¥°å™¨
def monitor_performance(func: Callable):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()

        try:
            result = await func(*args, **kwargs)
            return result
        finally:
            end_time = time.time()
            execution_time = end_time - start_time

            if execution_time > api_performance_optimizer.thresholds.response_time_warning:
                logger.warning(f"å‡½æ•° {func.__name__} æ‰§è¡Œæ—¶é—´è¿‡é•¿: {execution_time:.3f}s")

    return wrapper

# æ‰¹é‡è¯·æ±‚ä¼˜åŒ–å™¨
class BatchRequestOptimizer:
    """æ‰¹é‡è¯·æ±‚ä¼˜åŒ–å™¨"""

    def __init__(self, max_concurrent: int = 10):
        self.semaphore = asyncio.Semaphore(max_concurrent)

    async def execute_batch_requests(
        self,
        requests: List[Callable],
        timeout: Optional[float] = None
    ) -> List[Any]:
        """æ‰§è¡Œæ‰¹é‡è¯·æ±‚"""
        async def execute_single(request_func):
            async with self.semaphore:
                try:
                    if timeout:
                        return await asyncio.wait_for(request_func(), timeout=timeout)
                    else:
                        return await request_func()
                except Exception as e:
                    logger.error(f"æ‰¹é‡è¯·æ±‚æ‰§è¡Œå¤±è´¥: {e}")
                    return None

        tasks = [execute_single(req) for req in requests]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        return [r for r in results if r is not None]

# æµ‹è¯•å‡½æ•°
async def test_api_performance_optimization():
    """æµ‹è¯•APIæ€§èƒ½ä¼˜åŒ–åŠŸèƒ½"""
    print("ğŸš€ æµ‹è¯•APIæ€§èƒ½ä¼˜åŒ–åŠŸèƒ½...")

    # æµ‹è¯•ç¼“å­˜åŠŸèƒ½
    @cache_api_response(ttl=60, vary_on_params=["user_id"])
    async def test_api_function(user_id: str, data: str):
        await asyncio.sleep(0.1)  # æ¨¡æ‹ŸAPIè°ƒç”¨
        return {"user_id": user_id, "data": data, "timestamp": datetime.now().isoformat()}

    # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆæ— ç¼“å­˜ï¼‰
    start_time = time.time()
    result1 = await test_api_function("user123", "test_data")
    time1 = time.time() - start_time

    # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆæœ‰ç¼“å­˜ï¼‰
    start_time = time.time()
    result2 = await test_api_function("user123", "test_data")
    time2 = time.time() - start_time

    print(f"ç¬¬ä¸€æ¬¡è°ƒç”¨è€—æ—¶: {time1:.3f}ç§’")
    print(f"ç¬¬äºŒæ¬¡è°ƒç”¨è€—æ—¶: {time2:.3f}ç§’")
    print(f"æ€§èƒ½æå‡: {((time1 - time2) / time1 * 100):.1f}%")

    # è·å–æ€§èƒ½æŠ¥å‘Š
    report = api_performance_optimizer.get_performance_report()
    print(f"\nğŸ“Š æ€§èƒ½æŠ¥å‘Š:")
    print(f"æ€»è¯·æ±‚æ•°: {report['summary']['total_requests']}")
    print(f"å¹³å‡å“åº”æ—¶é—´: {report['summary']['avg_response_time']:.3f}ç§’")
    print(f"ç¼“å­˜å‘½ä¸­ç‡: {report['cache_stats']['hit_rate_percent']:.1f}%")

    # è·å–ç¼“å­˜ç»Ÿè®¡
    cache_stats = api_performance_optimizer.cache_manager.get_cache_stats()
    print(f"\nğŸ’¾ ç¼“å­˜ç»Ÿè®¡:")
    print(f"ç¼“å­˜ç±»å‹: {cache_stats['cache_type']}")
    print(f"å‘½ä¸­æ¬¡æ•°: {cache_stats['hits']}")
    print(f"æœªå‘½ä¸­æ¬¡æ•°: {cache_stats['misses']}")
    print(f"å‘½ä¸­ç‡: {cache_stats['hit_rate_percent']:.1f}%")

if __name__ == "__main__":
    import asyncio
    asyncio.run(test_api_performance_optimization())