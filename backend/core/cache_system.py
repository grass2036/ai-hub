"""
ç¼“å­˜ç³»ç»Ÿé…ç½®å’Œä¼˜åŒ–æ¨¡å—
Week 6 Day 2: æ€§èƒ½ä¼˜åŒ–å’Œè°ƒä¼˜ - ç¼“å­˜ç³»ç»Ÿä¼˜åŒ–
å®ç°å¤šå±‚ç¼“å­˜ã€æ™ºèƒ½å¤±æ•ˆã€åˆ†å¸ƒå¼ç¼“å­˜ã€ç¼“å­˜é¢„çƒ­ç­‰åŠŸèƒ½
"""

import asyncio
import json
import time
import hashlib
import logging
import pickle
import gzip
from typing import Dict, List, Any, Optional, Union, Callable, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from contextlib import asynccontextmanager
import aioredis
import aiomcache
from cachetools import TTLCache, LRUCache
from functools import wraps
import numpy as np

from backend.config.settings import get_settings

settings = get_settings()
logger = logging.getLogger(__name__)

class CacheLevel(Enum):
    """ç¼“å­˜å±‚çº§"""
    L1_MEMORY = "L1_MEMORY"  # å†…å­˜ç¼“å­˜
    L2_REDIS = "L2_REDIS"    # Redisç¼“å­˜
    L3_DISTRIBUTED = "L3_DISTRIBUTED"  # åˆ†å¸ƒå¼ç¼“å­˜

class CachePolicy(Enum):
    """ç¼“å­˜ç­–ç•¥"""
    LRU = "LRU"  # æœ€è¿‘æœ€å°‘ä½¿ç”¨
    LFU = "LFU"  # æœ€å°‘ä½¿ç”¨é¢‘ç‡
    TTL = "TTL"  # ç”Ÿå­˜æ—¶é—´
    FIFO = "FIFO"  # å…ˆè¿›å…ˆå‡º

@dataclass
class CacheConfig:
    """ç¼“å­˜é…ç½®"""
    ttl: int = 300  # ç”Ÿå­˜æ—¶é—´ï¼ˆç§’ï¼‰
    max_size: int = 1000  # æœ€å¤§æ¡ç›®æ•°
    compression: bool = True  # æ˜¯å¦å‹ç¼©
    serialization: str = "json"  # åºåˆ—åŒ–æ–¹å¼
    cache_levels: List[CacheLevel] = field(default_factory=lambda: [CacheLevel.L1_MEMORY, CacheLevel.L2_REDIS])
    eviction_policy: CachePolicy = CachePolicy.LRU
    enable_stats: bool = True

@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    key: str
    value: Any
    created_at: datetime
    expires_at: datetime
    access_count: int = 0
    last_accessed: datetime = field(default_factory=datetime.now)
    size_bytes: int = 0
    compressed: bool = False
    version: int = 1

@dataclass
class CacheStats:
    """ç¼“å­˜ç»Ÿè®¡"""
    hits: int = 0
    misses: int = 0
    sets: int = 0
    deletes: int = 0
    evictions: int = 0
    size_bytes: int = 0
    entry_count: int = 0

class MemoryCache:
    """å†…å­˜ç¼“å­˜å®ç°"""

    def __init__(self, config: CacheConfig):
        self.config = config
        self.cache: Dict[str, CacheEntry] = {}
        self.access_order = []  # LRUè®¿é—®é¡ºåº
        self.stats = CacheStats()
        self._lock = asyncio.Lock()

    def _generate_cache_key(self, key: str, version: int = 1) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"{key}:v{version}"

    async def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼"""
        async with self._lock:
            entry = self.cache.get(key)
            if entry is None:
                self.stats.misses += 1
                return None

            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if datetime.now() > entry.expires_at:
                await self.delete(key)
                self.stats.misses += 1
                return None

            # æ›´æ–°è®¿é—®ä¿¡æ¯
            entry.access_count += 1
            entry.last_accessed = datetime.now()
            self._update_access_order(key)

            self.stats.hits += 1
            return self._deserialize_value(entry.value, entry.compressed)

    async def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """è®¾ç½®ç¼“å­˜å€¼"""
        async with self._lock:
            try:
                current_time = datetime.now()
                actual_ttl = ttl or self.config.ttl
                expires_at = current_time + timedelta(seconds=actual_ttl)

                # åºåˆ—åŒ–å€¼
                serialized_value, compressed = self._serialize_value(value)

                entry = CacheEntry(
                    key=key,
                    value=serialized_value,
                    created_at=current_time,
                    expires_at=expires_at,
                    size_bytes=len(serialized_value),
                    compressed=compressed
                )

                # æ£€æŸ¥å®¹é‡é™åˆ¶
                if len(self.cache) >= self.config.max_size:
                    await self._evict_entries()

                self.cache[key] = entry
                self._update_access_order(key)

                self.stats.sets += 1
                self.stats.entry_count = len(self.cache)
                self.stats.size_bytes += entry.size_bytes

                return True

            except Exception as e:
                logger.error(f"å†…å­˜ç¼“å­˜è®¾ç½®å¤±è´¥: {e}")
                return False

    async def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜å€¼"""
        async with self._lock:
            if key in self.cache:
                entry = self.cache[key]
                del self.cache[key]
                self.access_order.remove(key)
                self.stats.deletes += 1
                self.stats.entry_count = len(self.cache)
                self.stats.size_bytes -= entry.size_bytes
                return True
            return False

    async def clear(self) -> bool:
        """æ¸…ç©ºç¼“å­˜"""
        async with self._lock:
            self.cache.clear()
            self.access_order.clear()
            self.stats = CacheStats()
            return True

    def _update_access_order(self, key: str):
        """æ›´æ–°è®¿é—®é¡ºåº"""
        if key in self.access_order:
            self.access_order.remove(key)
        self.access_order.append(key)

    async def _evict_entries(self):
        """é©±é€ç¼“å­˜æ¡ç›®"""
        if not self.access_order:
            return

        if self.config.eviction_policy == CachePolicy.LRU:
            # åˆ é™¤æœ€å°‘ä½¿ç”¨çš„æ¡ç›®
            evict_count = max(1, len(self.cache) // 10)  # åˆ é™¤10%
            for _ in range(evict_count):
                if self.access_order:
                    key_to_evict = self.access_order.pop(0)
                    if key_to_evict in self.cache:
                        entry = self.cache[key_to_evict]
                        del self.cache[key_to_evict]
                        self.stats.evictions += 1
                        self.stats.size_bytes -= entry.size_bytes

        elif self.config.eviction_policy == CachePolicy.FIFO:
            # åˆ é™¤æœ€æ—§çš„æ¡ç›®
            oldest_key = min(self.cache.keys(), key=lambda k: self.cache[k].created_at)
            entry = self.cache[oldest_key]
            del self.cache[oldest_key]
            self.access_order.remove(oldest_key)
            self.stats.evictions += 1
            self.stats.size_bytes -= entry.size_bytes

    def _serialize_value(self, value: Any) -> Tuple[bytes, bool]:
        """åºåˆ—åŒ–å€¼"""
        try:
            if self.config.serialization == "json":
                serialized = json.dumps(value, default=str).encode('utf-8')
            elif self.config.serialization == "pickle":
                serialized = pickle.dumps(value)
            else:
                serialized = str(value).encode('utf-8')

            # å‹ç¼©
            if self.config.compression and len(serialized) > 1024:  # åªå‹ç¼©å¤§äº1KBçš„æ•°æ®
                compressed = gzip.compress(serialized)
                if len(compressed) < len(serialized):  # åªåœ¨å‹ç¼©æœ‰æ•ˆæ—¶ä½¿ç”¨
                    return compressed, True

            return serialized, False

        except Exception as e:
            logger.error(f"åºåˆ—åŒ–å¤±è´¥: {e}")
            return str(value).encode('utf-8'), False

    def _deserialize_value(self, serialized: bytes, compressed: bool) -> Any:
        """ååºåˆ—åŒ–å€¼"""
        try:
            # è§£å‹ç¼©
            if compressed:
                serialized = gzip.decompress(serialized)

            if self.config.serialization == "json":
                return json.loads(serialized.decode('utf-8'))
            elif self.config.serialization == "pickle":
                return pickle.loads(serialized)
            else:
                return serialized.decode('utf-8')

        except Exception as e:
            logger.error(f"ååºåˆ—åŒ–å¤±è´¥: {e}")
            return serialized.decode('utf-8', errors='ignore')

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total_requests = self.stats.hits + self.stats.misses
        hit_rate = (self.stats.hits / total_requests * 100) if total_requests > 0 else 0

        return {
            "type": "memory",
            "hits": self.stats.hits,
            "misses": self.stats.misses,
            "hit_rate_percent": round(hit_rate, 2),
            "sets": self.stats.sets,
            "deletes": self.stats.deletes,
            "evictions": self.stats.evictions,
            "entry_count": self.stats.entry_count,
            "size_bytes": self.stats.size_bytes,
            "max_size": self.config.max_size
        }

class RedisCache:
    """Redisç¼“å­˜å®ç°"""

    def __init__(self, config: CacheConfig):
        self.config = config
        self.redis_client: Optional[aioredis.Redis] = None
        self.stats = CacheStats()
        self._lock = asyncio.Lock()

    async def _get_client(self) -> aioredis.Redis:
        """è·å–Rediså®¢æˆ·ç«¯"""
        if self.redis_client is None:
            await self._connect()
        return self.redis_client

    async def _connect(self):
        """è¿æ¥Redis"""
        try:
            self.redis_client = await aioredis.from_url(
                f"redis://{getattr(settings, 'redis_host', 'localhost')}:{getattr(settings, 'redis_port', 6379)}",
                encoding="utf-8",
                decode_responses=False  # ä½¿ç”¨äºŒè¿›åˆ¶æ•°æ®
            )
            await self.redis_client.ping()
            logger.info("Redisè¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"Redisè¿æ¥å¤±è´¥: {e}")
            raise

    async def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼"""
        try:
            client = await self._get_client()
            serialized = await client.get(key)

            if serialized is None:
                self.stats.misses += 1
                return None

            # è§£æå…ƒæ•°æ®
            metadata_length = int.from_bytes(serialized[:4], byteorder='big')
            metadata = json.loads(serialized[4:4+metadata_length].decode('utf-8'))
            value_bytes = serialized[4+metadata_length:]

            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if datetime.now() > datetime.fromisoformat(metadata['expires_at']):
                await self.delete(key)
                self.stats.misses += 1
                return None

            self.stats.hits += 1
            return self._deserialize_value(value_bytes, metadata.get('compressed', False))

        except Exception as e:
            logger.error(f"Redisè·å–å¤±è´¥: {e}")
            self.stats.misses += 1
            return None

    async def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """è®¾ç½®ç¼“å­˜å€¼"""
        try:
            client = await self._get_client()
            actual_ttl = ttl or self.config.ttl
            expires_at = datetime.now() + timedelta(seconds=actual_ttl)

            # åºåˆ—åŒ–å€¼
            value_bytes, compressed = self._serialize_value(value)

            # æ„å»ºå…ƒæ•°æ®
            metadata = {
                'expires_at': expires_at.isoformat(),
                'compressed': compressed,
                'created_at': datetime.now().isoformat(),
                'version': 1
            }
            metadata_bytes = json.dumps(metadata).encode('utf-8')
            metadata_length = len(metadata_bytes).to_bytes(4, byteorder='big')

            # ç»„åˆæ•°æ®
            serialized = metadata_length + metadata_bytes + value_bytes

            success = await client.setex(key, actual_ttl, serialized)
            if success:
                self.stats.sets += 1
                return True

        except Exception as e:
            logger.error(f"Redisè®¾ç½®å¤±è´¥: {e}")

        return False

    async def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜å€¼"""
        try:
            client = await self._get_client()
            result = await client.delete(key)
            if result > 0:
                self.stats.deletes += 1
                return True
        except Exception as e:
            logger.error(f"Redisåˆ é™¤å¤±è´¥: {e}")

        return False

    async def clear_pattern(self, pattern: str) -> int:
        """æŒ‰æ¨¡å¼åˆ é™¤ç¼“å­˜"""
        try:
            client = await self._get_client()
            keys = await client.keys(pattern)
            if keys:
                deleted_count = await client.delete(*keys)
                self.stats.deletes += deleted_count
                return deleted_count
        except Exception as e:
            logger.error(f"Redisæ¨¡å¼åˆ é™¤å¤±è´¥: {e}")

        return 0

    def _serialize_value(self, value: Any) -> Tuple[bytes, bool]:
        """åºåˆ—åŒ–å€¼ï¼ˆä¸MemoryCacheç›¸åŒï¼‰"""
        try:
            if self.config.serialization == "json":
                serialized = json.dumps(value, default=str).encode('utf-8')
            elif self.config.serialization == "pickle":
                serialized = pickle.dumps(value)
            else:
                serialized = str(value).encode('utf-8')

            if self.config.compression and len(serialized) > 1024:
                compressed = gzip.compress(serialized)
                if len(compressed) < len(serialized):
                    return compressed, True

            return serialized, False

        except Exception as e:
            logger.error(f"åºåˆ—åŒ–å¤±è´¥: {e}")
            return str(value).encode('utf-8'), False

    def _deserialize_value(self, serialized: bytes, compressed: bool) -> Any:
        """ååºåˆ—åŒ–å€¼ï¼ˆä¸MemoryCacheç›¸åŒï¼‰"""
        try:
            if compressed:
                serialized = gzip.decompress(serialized)

            if self.config.serialization == "json":
                return json.loads(serialized.decode('utf-8'))
            elif self.config.serialization == "pickle":
                return pickle.loads(serialized)
            else:
                return serialized.decode('utf-8')

        except Exception as e:
            logger.error(f"ååºåˆ—åŒ–å¤±è´¥: {e}")
            return serialized.decode('utf-8', errors='ignore')

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total_requests = self.stats.hits + self.stats.misses
        hit_rate = (self.stats.hits / total_requests * 100) if total_requests > 0 else 0

        return {
            "type": "redis",
            "hits": self.stats.hits,
            "misses": self.stats.misses,
            "hit_rate_percent": round(hit_rate, 2),
            "sets": self.stats.sets,
            "deletes": self.stats.deletes,
            "connected": self.redis_client is not None
        }

class MultiLevelCache:
    """å¤šå±‚ç¼“å­˜ç³»ç»Ÿ"""

    def __init__(self, config: CacheConfig):
        self.config = config
        self.caches = {}
        self.stats = CacheStats()

        # åˆå§‹åŒ–ç¼“å­˜å±‚çº§
        for level in config.cache_levels:
            if level == CacheLevel.L1_MEMORY:
                self.caches[level] = MemoryCache(config)
            elif level == CacheLevel.L2_REDIS:
                self.caches[level] = RedisCache(config)

    async def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼ï¼ˆæŒ‰å±‚çº§æŸ¥æ‰¾ï¼‰"""
        for level in self.config.cache_levels:
            if level in self.caches:
                cache = self.caches[level]
                value = await cache.get(key)
                if value is not None:
                    # å°†å€¼å›å¡«åˆ°æ›´é«˜çº§çš„ç¼“å­˜
                    await self._backfill_higher_levels(key, value, level)
                    self.stats.hits += 1
                    return value

        self.stats.misses += 1
        return None

    async def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """è®¾ç½®ç¼“å­˜å€¼ï¼ˆè®¾ç½®åˆ°æ‰€æœ‰å±‚çº§ï¼‰"""
        success = True
        for cache in self.caches.values():
            result = await cache.set(key, value, ttl)
            success = success and result

        if success:
            self.stats.sets += 1

        return success

    async def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜å€¼ï¼ˆä»æ‰€æœ‰å±‚çº§åˆ é™¤ï¼‰"""
        success = True
        for cache in self.caches.values():
            result = await cache.delete(key)
            success = success and result

        if success:
            self.stats.deletes += 1

        return success

    async def clear_pattern(self, pattern: str) -> Dict[CacheLevel, int]:
        """æŒ‰æ¨¡å¼æ¸…ç†ç¼“å­˜"""
        results = {}
        for level, cache in self.caches.items():
            if hasattr(cache, 'clear_pattern'):
                count = await cache.clear_pattern(pattern)
                results[level] = count
            else:
                results[level] = 0

        return results

    async def _backfill_higher_levels(self, key: str, value: Any, found_level: CacheLevel):
        """å°†å€¼å›å¡«åˆ°æ›´é«˜çº§çš„ç¼“å­˜"""
        found_index = self.config.cache_levels.index(found_level)
        for i in range(found_index):
            higher_level = self.config.cache_levels[i]
            if higher_level in self.caches:
                await self.caches[higher_level].set(key, value, self.config.ttl // 2)  # æ›´çŸ­çš„TTL

    def get_stats(self) -> Dict[str, Any]:
        """è·å–å¤šå±‚ç¼“å­˜ç»Ÿè®¡"""
        total_requests = self.stats.hits + self.stats.misses
        hit_rate = (self.stats.hits / total_requests * 100) if total_requests > 0 else 0

        return {
            "multi_level": {
                "hits": self.stats.hits,
                "misses": self.stats.misses,
                "hit_rate_percent": round(hit_rate, 2),
                "sets": self.stats.sets,
                "deletes": self.stats.deletes
            },
            "levels": {
                level.value: cache.get_stats()
                for level, cache in self.caches.items()
            }
        }

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self):
        self.caches: Dict[str, MultiLevelCache] = {}
        self.default_config = CacheConfig()

    def register_cache(self, name: str, config: CacheConfig = None) -> MultiLevelCache:
        """æ³¨å†Œç¼“å­˜å®ä¾‹"""
        if config is None:
            config = self.default_config

        cache = MultiLevelCache(config)
        self.caches[name] = cache
        logger.info(f"æ³¨å†Œç¼“å­˜å®ä¾‹: {name}")
        return cache

    def get_cache(self, name: str) -> Optional[MultiLevelCache]:
        """è·å–ç¼“å­˜å®ä¾‹"""
        return self.caches.get(name)

    async def warm_up_cache(self, cache_name: str, data_loader: Callable, keys: List[str]):
        """ç¼“å­˜é¢„çƒ­"""
        cache = self.get_cache(cache_name)
        if not cache:
            logger.error(f"ç¼“å­˜å®ä¾‹ä¸å­˜åœ¨: {cache_name}")
            return

        logger.info(f"å¼€å§‹é¢„çƒ­ç¼“å­˜: {cache_name}, æ¡ç›®æ•°: {len(keys)}")

        tasks = []
        for key in keys:
            task = self._load_and_cache(cache, data_loader, key)
            tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)
        success_count = sum(1 for r in results if not isinstance(r, Exception))

        logger.info(f"ç¼“å­˜é¢„çƒ­å®Œæˆ: {cache_name}, æˆåŠŸ: {success_count}/{len(keys)}")

    async def _load_and_cache(self, cache: MultiLevelCache, data_loader: Callable, key: str):
        """åŠ è½½å¹¶ç¼“å­˜å•ä¸ªæ¡ç›®"""
        try:
            value = await data_loader(key)
            if value is not None:
                await cache.set(key, value)
        except Exception as e:
            logger.error(f"ç¼“å­˜é¢„çƒ­å¤±è´¥ {key}: {e}")

    def get_all_stats(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰ç¼“å­˜ç»Ÿè®¡"""
        return {
            name: cache.get_stats()
            for name, cache in self.caches.items()
        }

# å…¨å±€ç¼“å­˜ç®¡ç†å™¨
cache_manager = CacheManager()

# è£…é¥°å™¨å‡½æ•°
def cache_result(
    cache_name: str = "default",
    ttl: int = None,
    key_generator: Callable = None,
    unless: Callable = None
):
    """ç¼“å­˜ç»“æœè£…é¥°å™¨"""
    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # æ£€æŸ¥æ¡ä»¶
            if unless and unless(*args, **kwargs):
                return await func(*args, **kwargs)

            # ç”Ÿæˆç¼“å­˜é”®
            if key_generator:
                cache_key = key_generator(*args, **kwargs)
            else:
                cache_key = f"{func.__name__}:{hash(str(args) + str(sorted(kwargs.items())))}"

            # è·å–ç¼“å­˜
            cache = cache_manager.get_cache(cache_name)
            if cache:
                cached_result = await cache.get(cache_key)
                if cached_result is not None:
                    return cached_result

            # æ‰§è¡Œå‡½æ•°
            result = await func(*args, **kwargs)

            # ç¼“å­˜ç»“æœ
            if cache:
                await cache.set(cache_key, result, ttl)

            return result

        return wrapper
    return decorator

# æµ‹è¯•å‡½æ•°
async def test_cache_system():
    """æµ‹è¯•ç¼“å­˜ç³»ç»ŸåŠŸèƒ½"""
    print("ğŸš€ æµ‹è¯•ç¼“å­˜ç³»ç»ŸåŠŸèƒ½...")

    # æ³¨å†Œç¼“å­˜å®ä¾‹
    default_cache = cache_manager.register_cache("default", CacheConfig(ttl=60, max_size=100))
    user_cache = cache_manager.register_cache("users", CacheConfig(ttl=300, max_size=500))

    # æµ‹è¯•åŸºæœ¬ç¼“å­˜æ“ä½œ
    await default_cache.set("test_key", {"data": "test_value", "timestamp": datetime.now().isoformat()})
    cached_value = await default_cache.get("test_key")
    print(f"ç¼“å­˜æµ‹è¯• - è·å–å€¼: {cached_value}")

    # æµ‹è¯•å¤šå±‚ç¼“å­˜
    await user_cache.set("user:123", {"name": "å¼ ä¸‰", "age": 30})
    user_data = await user_cache.get("user:123")
    print(f"ç”¨æˆ·ç¼“å­˜æµ‹è¯•: {user_data}")

    # æµ‹è¯•ç¼“å­˜è£…é¥°å™¨
    @cache_result(cache_name="default", ttl=120)
    async def expensive_function(n: int) -> int:
        await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ
        return n * n

    # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆæ— ç¼“å­˜ï¼‰
    start_time = time.time()
    result1 = await expensive_function(10)
    time1 = time.time() - start_time

    # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆæœ‰ç¼“å­˜ï¼‰
    start_time = time.time()
    result2 = await expensive_function(10)
    time2 = time.time() - start_time

    print(f"\nè£…é¥°å™¨æµ‹è¯•:")
    print(f"ç¬¬ä¸€æ¬¡è°ƒç”¨è€—æ—¶: {time1:.3f}ç§’ï¼Œç»“æœ: {result1}")
    print(f"ç¬¬äºŒæ¬¡è°ƒç”¨è€—æ—¶: {time2:.3f}ç§’ï¼Œç»“æœ: {result2}")
    print(f"æ€§èƒ½æå‡: {((time1 - time2) / time1 * 100):.1f}%")

    # è·å–ç¼“å­˜ç»Ÿè®¡
    stats = cache_manager.get_all_stats()
    print(f"\nğŸ“Š ç¼“å­˜ç»Ÿè®¡:")
    for name, stat in stats.items():
        print(f"{name}: å‘½ä¸­ç‡ {stat['multi_level']['hit_rate_percent']:.1f}%")

if __name__ == "__main__":
    import asyncio
    asyncio.run(test_cache_system())