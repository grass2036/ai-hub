"""
AIæ¨¡å‹ç®¡ç†API
"""

from fastapi import APIRouter, HTTPException, Query
from pydantic import BaseModel, Field
from typing import Optional, Dict, Any, List
from backend.core.ai_service import ai_manager
from backend.core.cache_service import cache_result

router = APIRouter()


class ModelInfo(BaseModel):
    """æ¨¡å‹ä¿¡æ¯æ¨¡å‹"""
    id: str = Field(..., description="æ¨¡å‹ID")
    name: str = Field(..., description="æ¨¡å‹åç§°")
    description: Optional[str] = Field(default=None, description="æ¨¡å‹æè¿°")
    context_length: Optional[int] = Field(default=None, description="ä¸Šä¸‹æ–‡é•¿åº¦")
    pricing: Optional[Dict[str, float]] = Field(default=None, description="å®šä»·ä¿¡æ¯")
    capabilities: Optional[List[str]] = Field(default=None, description="æ¨¡å‹èƒ½åŠ›")


class ModelsListResponse(BaseModel):
    """æ¨¡å‹åˆ—è¡¨å“åº”"""
    service: str = Field(..., description="AIæœåŠ¡åç§°")
    models: List[ModelInfo] = Field(..., description="æ¨¡å‹åˆ—è¡¨")
    total_count: int = Field(..., description="æ¨¡å‹æ€»æ•°")
    popular_models: List[str] = Field(..., description="çƒ­é—¨æ¨¡å‹")


class ServiceModelsResponse(BaseModel):
    """æœåŠ¡æ¨¡å‹å“åº”"""
    service: str = Field(..., description="æœåŠ¡åç§°")
    available: bool = Field(..., description="æœåŠ¡æ˜¯å¦å¯ç”¨")
    models: Optional[List[ModelInfo]] = Field(default=None, description="æ¨¡å‹åˆ—è¡¨")
    error: Optional[str] = Field(default=None, description="é”™è¯¯ä¿¡æ¯")


@router.get("/models", response_model=List[ServiceModelsResponse])
@cache_result(ttl=300, key_prefix="models_list")  # ç¼“å­˜5åˆ†é’Ÿ
async def get_available_models():
    """
    è·å–æ‰€æœ‰AIæœåŠ¡çš„å¯ç”¨æ¨¡å‹
    """
    try:
        services = await ai_manager.list_available_services()
        results = []

        for service_name in services["services"]:
            try:
                models_data = await ai_manager.get_models(service_name)

                models = []
                for model in models_data.get("models", []):
                    models.append(ModelInfo(**model))

                results.append(ServiceModelsResponse(
                    service=service_name,
                    available=True,
                    models=models,
                    total_count=len(models)
                ))

            except Exception as e:
                results.append(ServiceModelsResponse(
                    service=service_name,
                    available=False,
                    error=str(e)
                ))

        return results

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}"
        )


@router.get("/models/{service}", response_model=ModelsListResponse)
@cache_result(ttl=600, key_prefix="service_models")  # ç¼“å­˜10åˆ†é’Ÿ
async def get_service_models(service: str):
    """
    è·å–æŒ‡å®šAIæœåŠ¡çš„æ¨¡å‹åˆ—è¡¨
    """
    try:
        models_data = await ai_manager.get_models(service)

        models = []
        for model in models_data.get("models", []):
            models.append(ModelInfo(**model))

        return ModelsListResponse(
            service=service,
            models=models,
            total_count=len(models),
            popular_models=models_data.get("popular_models", [])
        )

    except ValueError as e:
        raise HTTPException(
            status_code=400,
            detail=str(e)
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è·å– {service} æ¨¡å‹å¤±è´¥: {str(e)}"
        )


@router.get("/models/{service}/{model_id}")
async def get_model_details(service: str, model_id: str):
    """
    è·å–æŒ‡å®šæ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯
    """
    try:
        models_data = await ai_manager.get_models(service)

        # æŸ¥æ‰¾æŒ‡å®šæ¨¡å‹
        model_info = None
        for model in models_data.get("models", []):
            if model.get("id") == model_id:
                model_info = model
                break

        if not model_info:
            raise HTTPException(
                status_code=404,
                detail=f"æ¨¡å‹ {model_id} åœ¨æœåŠ¡ {service} ä¸­æœªæ‰¾åˆ°"
            )

        return {
            "service": service,
            "model": model_info,
            "usage_example": {
                "endpoint": f"/api/v1/chat/completions",
                "parameters": {
                    "model": model_id,
                    "messages": [{"role": "user", "content": "Hello!"}]
                }
            }
        }

    except ValueError as e:
        raise HTTPException(
            status_code=400,
            detail=str(e)
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è·å–æ¨¡å‹è¯¦æƒ…å¤±è´¥: {str(e)}"
        )


@router.get("/models/popular")
@cache_result(ttl=1800, key_prefix="popular_models")  # ç¼“å­˜30åˆ†é’Ÿ
async def get_popular_models():
    """
    è·å–çƒ­é—¨æ¨¡å‹æ¨è
    """
    try:
        popular_models = {
            "openrouter": [
                {
                    "id": "anthropic/claude-3.5-sonnet",
                    "name": "Claude 3.5 Sonnet",
                    "description": "æœ€æ–°çš„Claudeæ¨¡å‹ï¼Œæ€§èƒ½ä¼˜å¼‚",
                    "use_case": "é€šç”¨å¯¹è¯ã€ç¼–ç¨‹ã€å†™ä½œ"
                },
                {
                    "id": "openai/gpt-4o",
                    "name": "GPT-4o",
                    "description": "OpenAIæœ€æ–°æ¨¡å‹ï¼Œå¤šæ¨¡æ€èƒ½åŠ›å¼º",
                    "use_case": "å›¾åƒç†è§£ã€å¤æ‚æ¨ç†"
                },
                {
                    "id": "google/gemini-pro-1.5",
                    "name": "Gemini Pro 1.5",
                    "description": "Googleæœ€æ–°æ¨¡å‹ï¼Œé•¿æ–‡æœ¬å¤„ç†ä¼˜ç§€",
                    "use_case": "é•¿æ–‡æ¡£åˆ†æã€ä»£ç ç”Ÿæˆ"
                },
                {
                    "id": "meta-llama/llama-3.1-70b-instruct",
                    "name": "Llama 3.1 70B",
                    "description": "å¼€æºæ¨¡å‹ï¼Œæ€§ä»·æ¯”é«˜",
                    "use_case": "é¢„ç®—æœ‰é™çš„é€šç”¨ä»»åŠ¡"
                },
                {
                    "id": "microsoft/wizardlm-2-8x22b",
                    "name": "WizardLM-2 8x22B",
                    "description": "å…è´¹æ¨¡å‹ï¼Œé€‚åˆæµ‹è¯•",
                    "use_case": "å¼€å‘æµ‹è¯•ã€ç®€å•ä»»åŠ¡"
                }
            ],
            "gemini": [
                {
                    "id": "gemini-2.5-flash",
                    "name": "Gemini 2.5 Flash",
                    "description": "å¿«é€Ÿå“åº”çš„é€šç”¨æ¨¡å‹",
                    "use_case": "å®æ—¶å¯¹è¯ã€å¿«é€Ÿå“åº”"
                },
                {
                    "id": "gemini-2.5-pro",
                    "name": "Gemini 2.5 Pro",
                    "description": "é«˜æ€§èƒ½ä¸“ä¸šæ¨¡å‹",
                    "use_case": "å¤æ‚æ¨ç†ã€ä¸“ä¸šä»»åŠ¡"
                }
            ]
        }

        return {
            "popular_models": popular_models,
            "recommendations": {
                "best_overall": "anthropic/claude-3.5-sonnet",
                "fastest": "gemini-2.5-flash",
                "free_option": "microsoft/wizardlm-2-8x22b",
                "cost_effective": "meta-llama/llama-3.1-70b-instruct"
            },
            "usage_tips": [
                "ğŸ’¡ Claude 3.5 Sonnetï¼šæœ€é€‚åˆå¤æ‚å¯¹è¯å’Œç¼–ç¨‹ä»»åŠ¡",
                "ğŸ’¡ GPT-4oï¼šæœ€ä½³å¤šæ¨¡æ€èƒ½åŠ›ï¼Œæ”¯æŒå›¾åƒç†è§£",
                "ğŸ’¡ Gemini 2.5 Flashï¼šå“åº”é€Ÿåº¦æœ€å¿«ï¼Œé€‚åˆå®æ—¶åº”ç”¨",
                "ğŸ’¡ Llama 3.1ï¼šå¼€æºé€‰æ‹©ï¼Œæ•°æ®éšç§å‹å¥½",
                "ğŸ’¡ WizardLM-2ï¼šå…è´¹é€‰é¡¹ï¼Œé€‚åˆå¼€å‘å’Œæµ‹è¯•"
            ]
        }

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è·å–çƒ­é—¨æ¨¡å‹å¤±è´¥: {str(e)}"
        )


@router.get("/models/recommendations")
async def get_model_recommendations(
    task_type: str = Query(..., description="ä»»åŠ¡ç±»å‹: chat, coding, writing, analysis, image"),
    budget: str = Query(default="balanced", description="é¢„ç®—çº§åˆ«: free, low, balanced, high")
):
    """
    æ ¹æ®ä»»åŠ¡ç±»å‹å’Œé¢„ç®—è·å–æ¨¡å‹æ¨è
    """
    try:
        recommendations = {
            "chat": {
                "free": ["microsoft/wizardlm-2-8x22b"],
                "low": ["meta-llama/llama-3.1-70b-instruct", "google/gemini-pro-1.5"],
                "balanced": ["anthropic/claude-3.5-sonnet", "openai/gpt-4o-mini"],
                "high": ["anthropic/claude-3.5-sonnet", "openai/gpt-4o"]
            },
            "coding": {
                "free": ["microsoft/wizardlm-2-8x22b"],
                "low": ["meta-llama/llama-3.1-70b-instruct"],
                "balanced": ["anthropic/claude-3.5-sonnet", "openai/gpt-4o"],
                "high": ["anthropic/claude-3.5-sonnet", "openai/gpt-4o"]
            },
            "writing": {
                "free": ["microsoft/wizardlm-2-8x22b"],
                "low": ["meta-llama/llama-3.1-70b-instruct"],
                "balanced": ["anthropic/claude-3.5-sonnet", "google/gemini-pro-1.5"],
                "high": ["anthropic/claude-3.5-sonnet", "openai/gpt-4o"]
            },
            "analysis": {
                "free": ["microsoft/wizardlm-2-8x22b"],
                "low": ["google/gemini-pro-1.5"],
                "balanced": ["anthropic/claude-3.5-sonnet", "openai/gpt-4o"],
                "high": ["anthropic/claude-3.5-sonnet", "openai/gpt-4o"]
            },
            "image": {
                "free": [],
                "low": ["openai/gpt-4o-mini"],
                "balanced": ["openai/gpt-4o"],
                "high": ["openai/gpt-4o", "anthropic/claude-3.5-sonnet"]
            }
        }

        task_recommendations = recommendations.get(task_type, {})
        budget_recommendations = task_recommendations.get(budget, [])

        return {
            "task_type": task_type,
            "budget": budget,
            "recommended_models": budget_recommendations,
            "alternative_budgets": {
                b: task_recommendations.get(b, [])
                for b in ["free", "low", "balanced", "high"]
                if b != budget and task_recommendations.get(b)
            }
        }

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è·å–æ¨¡å‹æ¨èå¤±è´¥: {str(e)}"
        )