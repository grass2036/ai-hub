# Week 8 - æ€§èƒ½ä¼˜åŒ–ä¸ç›‘æ§ä½“ç³»å»ºè®¾

## å¼€ï¿½ï¿½ï¿½ç›®æ ‡
å»ºç«‹å®Œæ•´çš„æ€§èƒ½ç›‘æ§ä½“ç³»ï¼Œå®ç°ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–ï¼Œç¡®ä¿AI Hubå¹³å°åœ¨é«˜å¹¶å‘åœºæ™¯ä¸‹çš„ç¨³å®šæ€§å’Œå“åº”é€Ÿåº¦ã€‚

## Day 1 - æ€§èƒ½ç›‘æ§åŸºç¡€è®¾æ–½

### ä¸Šåˆä»»åŠ¡ï¼šç›‘æ§æ•°æ®é‡‡é›†ç³»ç»Ÿ

#### 1. åº”ç”¨æ€§èƒ½ç›‘æ§ (APM)
```typescript
// frontend/src/monitoring/PerformanceMonitor.tsx
interface PerformanceMetrics {
  pageLoad: number;
  apiResponse: number;
  renderTime: number;
  userInteraction: number;
  errorRate: number;
}

export class PerformanceMonitor {
  private metrics: PerformanceMetrics[] = [];

  // é¡µé¢æ€§èƒ½ç›‘æ§
  monitorPageLoad(): void {
    const navigation = performance.getEntriesByType('navigation')[0] as PerformanceNavigationTiming;
    const pageLoad = navigation.loadEventEnd - navigation.navigationStart;

    this.recordMetric('pageLoad', pageLoad);
  }

  // APIæ€§èƒ½ç›‘æ§
  monitorApiCall(endpoint: string, duration: number): void {
    this.recordMetric('apiResponse', duration, { endpoint });
  }

  // ç”¨æˆ·äº¤äº’ç›‘æ§
  monitorUserInteraction(action: string): void {
    const startTime = performance.now();

    return () => {
      const duration = performance.now() - startTime;
      this.recordMetric('userInteraction', duration, { action });
    };
  }
}
```

#### 2. ç³»ç»ŸæŒ‡æ ‡ç›‘æ§
```python
# backend/monitoring/system_monitor.py
import psutil
import asyncio
from typing import Dict, List
from datetime import datetime

class SystemMonitor:
    def __init__(self):
        self.metrics_history = []
        self.alerts = []

    async def collect_system_metrics(self) -> Dict:
        """æ”¶é›†ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
        return {
            'timestamp': datetime.utcnow().isoformat(),
            'cpu': {
                'usage_percent': psutil.cpu_percent(interval=1),
                'count': psutil.cpu_count(),
                'load_avg': psutil.getloadavg()
            },
            'memory': {
                'total': psutil.virtual_memory().total,
                'available': psutil.virtual_memory().available,
                'percent': psutil.virtual_memory().percent,
                'used': psutil.virtual_memory().used
            },
            'disk': {
                'total': psutil.disk_usage('/').total,
                'used': psutil.disk_usage('/').used,
                'free': psutil.disk_usage('/').free,
                'percent': psutil.disk_usage('/').percent
            },
            'network': self._get_network_stats()
        }

    def _get_network_stats(self) -> Dict:
        """è·å–ç½‘ç»œç»Ÿè®¡ä¿¡æ¯"""
        net_io = psutil.net_io_counters()
        return {
            'bytes_sent': net_io.bytes_sent,
            'bytes_recv': net_io.bytes_recv,
            'packets_sent': net_io.packets_sent,
            'packets_recv': net_io.packets_recv
        }
```

#### 3. ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§
```python
# backend/monitoring/business_monitor.py
from dataclasses import dataclass
from typing import Dict, Any
import json

@dataclass
class BusinessMetric:
    name: str
    value: float
    unit: str
    timestamp: datetime
    tags: Dict[str, str] = None

class BusinessMonitor:
    def __init__(self):
        self.metrics = []

    def track_api_usage(self, endpoint: str, user_id: str, response_time: float):
        """è¿½è¸ªAPIä½¿ç”¨æƒ…å†µ"""
        metric = BusinessMetric(
            name='api_usage',
            value=response_time,
            unit='ms',
            timestamp=datetime.utcnow(),
            tags={
                'endpoint': endpoint,
                'user_id': user_id,
                'status': 'success'
            }
        )
        self.metrics.append(metric)

    def track_ai_model_usage(self, model: str, tokens: int, cost: float):
        """è¿½è¸ªAIæ¨¡å‹ä½¿ç”¨æƒ…å†µ"""
        metric = BusinessMetric(
            name='ai_model_usage',
            value=cost,
            unit='USD',
            timestamp=datetime.utcnow(),
            tags={
                'model': model,
                'tokens': str(tokens)
            }
        )
        self.metrics.append(metric)

    def track_user_session(self, user_id: str, session_duration: float):
        """è¿½è¸ªç”¨æˆ·ä¼šè¯"""
        metric = BusinessMetric(
            name='user_session_duration',
            value=session_duration,
            unit='seconds',
            timestamp=datetime.utcnow(),
            tags={'user_id': user_id}
        )
        self.metrics.append(metric)
```

### ä¸‹åˆä»»åŠ¡ï¼šç›‘æ§æ•°æ®å­˜å‚¨

#### 1. æ—¶åºæ•°æ®åº“é…ç½®
```yaml
# deployment/monitoring/timescale-db.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: timescaledb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: timescaledb
  template:
    metadata:
      labels:
        app: timescaledb
    spec:
      containers:
      - name: timescaledb
        image: timescale/timescaledb:latest-pg14
        env:
        - name: POSTGRES_DB
          value: "monitoring"
        - name: POSTGRES_USER
          value: "monitoring_user"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-secrets
              key: timescaledb-password
        ports:
        - containerPort: 5432
        volumeMounts:
        - name: timescaledb-storage
          mountPath: /var/lib/postgresql/data
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
      volumes:
      - name: timescaledb-storage
        persistentVolumeClaim:
          claimName: timescaledb-pvc
```

#### 2. ç›‘æ§æ•°æ®æ¨¡å‹
```sql
-- migrations/005_monitoring_schema.sql

-- æ€§èƒ½æŒ‡æ ‡è¡¨
CREATE TABLE performance_metrics (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    metric_name VARCHAR(100) NOT NULL,
    value DOUBLE PRECISION NOT NULL,
    unit VARCHAR(20),
    tags JSONB,
    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- åˆ›å»ºæ—¶åºè¡¨
SELECT create_hypertable('performance_metrics', 'timestamp', chunk_time_interval => INTERVAL '1 hour');

-- ä¸šåŠ¡æŒ‡æ ‡è¡¨
CREATE TABLE business_metrics (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    metric_name VARCHAR(100) NOT NULL,
    value DOUBLE PRECISION NOT NULL,
    unit VARCHAR(20),
    user_id UUID,
    organization_id UUID,
    tags JSONB,
    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

SELECT create_hypertable('business_metrics', 'timestamp', chunk_time_interval => INTERVAL '1 hour');

-- ç³»ç»ŸæŒ‡æ ‡è¡¨
CREATE TABLE system_metrics (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    host_name VARCHAR(255) NOT NULL,
    cpu_usage DOUBLE PRECISION,
    memory_usage DOUBLE PRECISION,
    disk_usage DOUBLE PRECISION,
    network_io JSONB,
    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

SELECT create_hypertable('system_metrics', 'timestamp', chunk_time_interval => INTERVAL '5 minutes');

-- å‘Šè­¦è§„åˆ™è¡¨
CREATE TABLE alert_rules (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(200) NOT NULL,
    metric_name VARCHAR(100) NOT NULL,
    condition_operator VARCHAR(10) NOT NULL, -- '>', '<', '>=', '<=', '=', '!='
    threshold_value DOUBLE PRECISION NOT NULL,
    duration_minutes INTEGER DEFAULT 5,
    severity VARCHAR(20) NOT NULL, -- 'critical', 'warning', 'info'
    enabled BOOLEAN DEFAULT true,
    notification_channels JSONB,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- å‘Šè­¦è®°å½•è¡¨
CREATE TABLE alert_incidents (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    rule_id UUID NOT NULL REFERENCES alert_rules(id),
    status VARCHAR(20) NOT NULL DEFAULT 'active', -- 'active', 'resolved', 'suppressed'
    trigger_value DOUBLE PRECISION NOT NULL,
    triggered_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    resolved_at TIMESTAMPTZ,
    notes TEXT,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- ç´¢å¼•ä¼˜åŒ–
CREATE INDEX idx_performance_metrics_name_timestamp ON performance_metrics(metric_name, timestamp DESC);
CREATE INDEX idx_business_metrics_user_timestamp ON business_metrics(user_id, timestamp DESC);
CREATE INDEX idx_business_metrics_org_timestamp ON business_metrics(organization_id, timestamp DESC);
CREATE INDEX idx_system_metrics_host_timestamp ON system_metrics(host_name, timestamp DESC);
CREATE INDEX idx_alert_incidents_rule_status ON alert_incidents(rule_id, status);
```

### æ™šä¸Šä»»åŠ¡ï¼šå®æ—¶ç›‘æ§ä»ªè¡¨æ¿

#### 1. ç›‘æ§ä»ªè¡¨æ¿ç»„ä»¶
```typescript
// frontend/src/components/monitoring/MonitoringDashboard.tsx
import React, { useState, useEffect } from 'react';
import { LineChart, Line, XAxis, YAxis, CartesianGrid, Tooltip, ResponsiveContainer } from 'recharts';

interface MonitoringData {
  timestamp: string;
  cpu: number;
  memory: number;
  disk: number;
  responseTime: number;
  errorRate: number;
}

export const MonitoringDashboard: React.FC = () => {
  const [data, setData] = useState<MonitoringData[]>([]);
  const [loading, setLoading] = useState(true);
  const [timeRange, setTimeRange] = useState('1h');

  useEffect(() => {
    const fetchMonitoringData = async () => {
      try {
        const response = await fetch(`/api/v1/monitoring/metrics?range=${timeRange}`);
        const result = await response.json();
        setData(result.data);
      } catch (error) {
        console.error('Failed to fetch monitoring data:', error);
      } finally {
        setLoading(false);
      }
    };

    fetchMonitoringData();
    const interval = setInterval(fetchMonitoringData, 30000); // 30ç§’æ›´æ–°

    return () => clearInterval(interval);
  }, [timeRange]);

  if (loading) {
    return <div className="p-6">åŠ è½½ç›‘æ§æ•°æ®ä¸­...</div>;
  }

  return (
    <div className="p-6 space-y-6">
      <div className="flex justify-between items-center">
        <h1 className="text-2xl font-bold">ç³»ç»Ÿç›‘æ§</h1>
        <select
          value={timeRange}
          onChange={(e) => setTimeRange(e.target.value)}
          className="border rounded px-3 py-1"
        >
          <option value="5m">æœ€è¿‘5åˆ†é’Ÿ</option>
          <option value="1h">æœ€è¿‘1å°æ—¶</option>
          <option value="6h">æœ€è¿‘6å°æ—¶</option>
          <option value="24h">æœ€è¿‘24å°æ—¶</option>
        </select>
      </div>

      {/* ç³»ç»Ÿèµ„æºç›‘æ§ */}
      <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-4">
        <MetricCard title="CPUä½¿ç”¨ç‡" value={data[data.length - 1]?.cpu} unit="%" />
        <MetricCard title="å†…å­˜ä½¿ç”¨ç‡" value={data[data.length - 1]?.memory} unit="%" />
        <MetricCard title="ç£ç›˜ä½¿ç”¨ç‡" value={data[data.length - 1]?.disk} unit="%" />
        <MetricCard title="å“åº”æ—¶é—´" value={data[data.length - 1]?.responseTime} unit="ms" />
      </div>

      {/* æ€§èƒ½è¶‹åŠ¿å›¾ */}
      <div className="grid grid-cols-1 lg:grid-cols-2 gap-6">
        <div className="bg-white p-4 rounded-lg shadow">
          <h3 className="text-lg font-semibold mb-4">ç³»ç»Ÿèµ„æºè¶‹åŠ¿</h3>
          <ResponsiveContainer width="100%" height={300}>
            <LineChart data={data}>
              <CartesianGrid strokeDasharray="3 3" />
              <XAxis dataKey="timestamp" />
              <YAxis />
              <Tooltip />
              <Line type="monotone" dataKey="cpu" stroke="#8884d8" name="CPU" />
              <Line type="monotone" dataKey="memory" stroke="#82ca9d" name="å†…å­˜" />
              <Line type="monotone" dataKey="disk" stroke="#ffc658" name="ç£ç›˜" />
            </LineChart>
          </ResponsiveContainer>
        </div>

        <div className="bg-white p-4 rounded-lg shadow">
          <h3 className="text-lg font-semibold mb-4">å“åº”æ—¶é—´è¶‹åŠ¿</h3>
          <ResponsiveContainer width="100%" height={300}>
            <LineChart data={data}>
              <CartesianGrid strokeDasharray="3 3" />
              <XAxis dataKey="timestamp" />
              <YAxis />
              <Tooltip />
              <Line type="monotone" dataKey="responseTime" stroke="#ff7300" name="å“åº”æ—¶é—´" />
              <Line type="monotone" dataKey="errorRate" stroke="#ff0000" name="é”™è¯¯ç‡" />
            </LineChart>
          </ResponsiveContainer>
        </div>
      </div>
    </div>
  );
};

const MetricCard: React.FC<{ title: string; value: number; unit: string }> = ({ title, value, unit }) => {
  const getColor = (value: number) => {
    if (value < 50) return 'text-green-600';
    if (value < 80) return 'text-yellow-600';
    return 'text-red-600';
  };

  return (
    <div className="bg-white p-4 rounded-lg shadow">
      <h3 className="text-sm font-medium text-gray-500">{title}</h3>
      <p className={`text-2xl font-bold ${getColor(value)}`}>
        {value?.toFixed(1)}{unit}
      </p>
    </div>
  );
};
```

## Day 2 - æ™ºèƒ½å‘Šè­¦ç³»ç»Ÿ

### ä¸Šåˆä»»åŠ¡ï¼šå‘Šè­¦è§„åˆ™å¼•æ“

#### 1. å‘Šè­¦è§„åˆ™é…ç½®
```python
# backend/monitoring/alert_engine.py
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
import asyncio

@dataclass
class AlertCondition:
    metric_name: str
    operator: str  # '>', '<', '>=', '<=', '=', '!='
    threshold: float
    duration_minutes: int = 5
    severity: str = 'warning'  # 'critical', 'warning', 'info'

class AlertEngine:
    def __init__(self):
        self.rules: Dict[str, AlertCondition] = {}
        self.active_alerts: Dict[str, datetime] = {}

    def add_rule(self, rule_id: str, condition: AlertCondition):
        """æ·»åŠ å‘Šè­¦è§„åˆ™"""
        self.rules[rule_id] = condition

    def remove_rule(self, rule_id: str):
        """ç§»é™¤å‘Šè­¦è§„åˆ™"""
        if rule_id in self.rules:
            del self.rules[rule_id]
        if rule_id in self.active_alerts:
            del self.active_alerts[rule_id]

    async def evaluate_metric(self, metric_name: str, value: float, timestamp: datetime):
        """è¯„ä¼°æŒ‡æ ‡æ˜¯å¦è§¦å‘å‘Šè­¦"""
        for rule_id, condition in self.rules.items():
            if condition.metric_name != metric_name:
                continue

            if self._evaluate_condition(value, condition):
                await self._handle_alert_trigger(rule_id, condition, value, timestamp)
            else:
                await self._handle_alert_resolve(rule_id, condition, timestamp)

    def _evaluate_condition(self, value: float, condition: AlertCondition) -> bool:
        """è¯„ä¼°æ¡ä»¶æ˜¯å¦æ»¡è¶³"""
        operators = {
            '>': lambda a, b: a > b,
            '<': lambda a, b: a < b,
            '>=': lambda a, b: a >= b,
            '<=': lambda a, b: a <= b,
            '=': lambda a, b: a == b,
            '!=': lambda a, b: a != b,
        }

        return operators.get(condition.operator, lambda a, b: False)(value, condition.threshold)

    async def _handle_alert_trigger(self, rule_id: str, condition: AlertCondition,
                                  value: float, timestamp: datetime):
        """å¤„ç†å‘Šè­¦è§¦å‘"""
        if rule_id not in self.active_alerts:
            self.active_alerts[rule_id] = timestamp
            return

        # æ£€æŸ¥æŒç»­æ—¶é—´
        duration = timestamp - self.active_alerts[rule_id]
        if duration >= timedelta(minutes=condition.duration_minutes):
            await self._send_alert(rule_id, condition, value, timestamp)

    async def _handle_alert_resolve(self, rule_id: str, condition: AlertCondition, timestamp: datetime):
        """å¤„ç†å‘Šè­¦è§£å†³"""
        if rule_id in self.active_alerts:
            del self.active_alerts[rule_id]
            await self._send_resolution(rule_id, condition, timestamp)

    async def _send_alert(self, rule_id: str, condition: AlertCondition,
                         value: float, timestamp: datetime):
        """å‘é€å‘Šè­¦é€šçŸ¥"""
        alert_data = {
            'rule_id': rule_id,
            'metric_name': condition.metric_name,
            'severity': condition.severity,
            'value': value,
            'threshold': condition.threshold,
            'timestamp': timestamp.isoformat(),
            'message': f"{condition.metric_name} {condition.operator} {condition.threshold} (å½“å‰å€¼: {value})"
        }

        # å‘é€åˆ°ä¸åŒçš„é€šçŸ¥æ¸ é“
        await asyncio.gather(
            self._send_email_alert(alert_data),
            self._send_slack_alert(alert_data),
            self._send_webhook_alert(alert_data)
        )

    async def _send_email_alert(self, alert_data: Dict):
        """å‘é€é‚®ä»¶å‘Šè­¦"""
        # å®ç°é‚®ä»¶å‘é€é€»è¾‘
        pass

    async def _send_slack_alert(self, alert_data: Dict):
        """å‘é€Slackå‘Šè­¦"""
        # å®ç°Slacké€šçŸ¥é€»è¾‘
        pass

    async def _send_webhook_alert(self, alert_data: Dict):
        """å‘é€Webhookå‘Šè­¦"""
        # å®ç°Webhooké€šçŸ¥é€»è¾‘
        pass
```

#### 2. é¢„å®šä¹‰å‘Šè­¦è§„åˆ™
```python
# backend/monitoring/default_alert_rules.py

DEFAULT_ALERT_RULES = {
    'high_cpu_usage': AlertCondition(
        metric_name='cpu_usage',
        operator='>',
        threshold=80.0,
        duration_minutes=5,
        severity='warning'
    ),

    'critical_cpu_usage': AlertCondition(
        metric_name='cpu_usage',
        operator='>',
        threshold=95.0,
        duration_minutes=2,
        severity='critical'
    ),

    'high_memory_usage': AlertCondition(
        metric_name='memory_usage',
        operator='>',
        threshold=85.0,
        duration_minutes=5,
        severity='warning'
    ),

    'critical_memory_usage': AlertCondition(
        metric_name='memory_usage',
        operator='>',
        threshold=95.0,
        duration_minutes=2,
        severity='critical'
    ),

    'high_response_time': AlertCondition(
        metric_name='api_response_time',
        operator='>',
        threshold=2000.0,  # 2ç§’
        duration_minutes=3,
        severity='warning'
    ),

    'critical_response_time': AlertCondition(
        metric_name='api_response_time',
        operator='>',
        threshold=5000.0,  # 5ç§’
        duration_minutes=1,
        severity='critical'
    ),

    'high_error_rate': AlertCondition(
        metric_name='error_rate',
        operator='>',
        threshold=5.0,  # 5%
        duration_minutes=2,
        severity='warning'
    ),

    'critical_error_rate': AlertCondition(
        metric_name='error_rate',
        operator='>',
        threshold=10.0,  # 10%
        duration_minutes=1,
        severity='critical'
    ),

    'disk_space_low': AlertCondition(
        metric_name='disk_usage',
        operator='>',
        threshold=90.0,
        duration_minutes=5,
        severity='warning'
    ),

    'disk_space_critical': AlertCondition(
        metric_name='disk_usage',
        operator='>',
        threshold=95.0,
        duration_minutes=1,
        severity='critical'
    )
}
```

### ä¸‹åˆä»»åŠ¡ï¼šæœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹

#### 1. å¼‚å¸¸æ£€æµ‹æ¨¡å‹
```python
# backend/monitoring/anomaly_detection.py
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from typing import List, Dict, Tuple
import joblib
from datetime import datetime, timedelta

class AnomalyDetector:
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.training_data = {}

    def train_model(self, metric_name: str, historical_data: List[Dict]):
        """è®­ç»ƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹"""
        if len(historical_data) < 100:  # è‡³å°‘éœ€è¦100ä¸ªæ•°æ®ç‚¹
            return False

        # æå–ç‰¹å¾
        features = self._extract_features(historical_data)

        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        scaled_features = scaler.fit_transform(features)

        # è®­ç»ƒæ¨¡å‹
        model = IsolationForest(
            contamination=0.1,  # 10%çš„å¼‚å¸¸ç‡
            random_state=42,
            n_estimators=100
        )
        model.fit(scaled_features)

        # ä¿å­˜æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨
        self.models[metric_name] = model
        self.scalers[metric_name] = scaler
        self.training_data[metric_name] = historical_data[-1000:]  # ä¿ç•™æœ€è¿‘1000ä¸ªæ•°æ®ç‚¹

        return True

    def detect_anomaly(self, metric_name: str, current_data: Dict) -> Tuple[bool, float]:
        """æ£€æµ‹å¼‚å¸¸"""
        if metric_name not in self.models:
            return False, 0.0

        model = self.models[metric_name]
        scaler = self.scalers[metric_name]

        # æå–å½“å‰ç‰¹å¾
        features = self._extract_features([current_data])
        scaled_features = scaler.transform(features)

        # é¢„æµ‹å¼‚å¸¸åˆ†æ•° (-1è¡¨ç¤ºå¼‚å¸¸ï¼Œ1è¡¨ç¤ºæ­£å¸¸)
        anomaly_score = model.decision_function(scaled_features)[0]
        is_anomaly = model.predict(scaled_features)[0] == -1

        return is_anomaly, anomaly_score

    def _extract_features(self, data: List[Dict]) -> np.ndarray:
        """æå–ç‰¹å¾"""
        features = []

        for i, point in enumerate(data):
            # åŸºç¡€æŒ‡æ ‡
            basic_features = [
                point.get('value', 0),
                point.get('rate_of_change', 0),
                point.get('moving_avg_5', 0),
                point.get('moving_avg_15', 0),
                point.get('volatility', 0),
                point.get('hour_of_day', datetime.fromisoformat(point['timestamp']).hour),
                point.get('day_of_week', datetime.fromisoformat(point['timestamp']).weekday())
            ]

            # æ·»åŠ å†å²ç‰¹å¾
            if i > 0:
                prev_point = data[i-1]
                basic_features.extend([
                    point.get('value', 0) - prev_point.get('value', 0),  # ä¸€é˜¶å·®åˆ†
                    (point.get('value', 0) / max(prev_point.get('value', 1), 1)) - 1,  # å˜åŒ–ç‡
                ])
            else:
                basic_features.extend([0, 0])

            features.append(basic_features)

        return np.array(features)

    def save_models(self, file_path: str):
        """ä¿å­˜æ¨¡å‹"""
        joblib.dump({
            'models': self.models,
            'scalers': self.scalers,
            'training_data': self.training_data
        }, file_path)

    def load_models(self, file_path: str):
        """åŠ è½½æ¨¡å‹"""
        data = joblib.load(file_path)
        self.models = data.get('models', {})
        self.scalers = data.get('scalers', {})
        self.training_data = data.get('training_data', {})
```

#### 2. æ™ºèƒ½å‘Šè­¦ç­–ç•¥
```python
# backend/monitoring/smart_alerting.py
from typing import Dict, List, Optional
from datetime import datetime, timedelta
import statistics

class SmartAlerting:
    def __init__(self, anomaly_detector: AnomalyDetector):
        self.anomaly_detector = anomaly_detector
        self.alert_history = {}
        self.suppression_rules = {}

    def evaluate_smart_alert(self, metric_name: str, current_value: float,
                           timestamp: datetime) -> Optional[Dict]:
        """æ™ºèƒ½å‘Šè­¦è¯„ä¼°"""
        # æ£€æŸ¥æŠ‘åˆ¶è§„åˆ™
        if self._is_suppressed(metric_name, timestamp):
            return None

        # è·å–å†å²æ•°æ®
        historical_data = self._get_historical_data(metric_name, lookback_hours=24)

        if len(historical_data) < 10:
            return None

        # è®¡ç®—ç»Ÿè®¡ç‰¹å¾
        stats = self._calculate_statistics(historical_data)

        # æ£€æµ‹å¼‚å¸¸
        current_data = {
            'timestamp': timestamp.isoformat(),
            'value': current_value,
            'rate_of_change': self._calculate_rate_of_change(historical_data, current_value),
            'moving_avg_5': statistics.mean([p['value'] for p in historical_data[-5:]]),
            'moving_avg_15': statistics.mean([p['value'] for p in historical_data[-15:]]),
            'volatility': statistics.stdev([p['value'] for p in historical_data[-10:]]),
            'hour_of_day': timestamp.hour,
            'day_of_week': timestamp.weekday()
        }

        is_anomaly, anomaly_score = self.anomaly_detector.detect_anomaly(metric_name, current_data)

        if is_anomaly:
            alert = {
                'metric_name': metric_name,
                'current_value': current_value,
                'anomaly_score': anomaly_score,
                'severity': self._calculate_severity(current_value, stats),
                'timestamp': timestamp.isoformat(),
                'context': {
                    'historical_avg': stats['mean'],
                    'historical_std': stats['std'],
                    'percentile': self._calculate_percentile(current_value, historical_data)
                }
            }

            # è®°å½•å‘Šè­¦å†å²
            self._record_alert(metric_name, alert)

            return alert

        return None

    def _calculate_statistics(self, data: List[Dict]) -> Dict:
        """è®¡ç®—ç»Ÿè®¡ç‰¹å¾"""
        values = [point['value'] for point in data]
        return {
            'mean': statistics.mean(values),
            'median': statistics.median(values),
            'std': statistics.stdev(values) if len(values) > 1 else 0,
            'min': min(values),
            'max': max(values),
            'p95': np.percentile(values, 95),
            'p99': np.percentile(values, 99)
        }

    def _calculate_severity(self, current_value: float, stats: Dict) -> str:
        """è®¡ç®—å‘Šè­¦ä¸¥é‡ç¨‹åº¦"""
        # åŸºäºç»Ÿè®¡åˆ†å¸ƒè®¡ç®—ä¸¥é‡ç¨‹åº¦
        if current_value > stats.get('p99', 0):
            return 'critical'
        elif current_value > stats.get('p95', 0):
            return 'warning'
        else:
            return 'info'

    def _calculate_percentile(self, value: float, data: List[Dict]) -> float:
        """è®¡ç®—å½“å‰å€¼åœ¨å†å²æ•°æ®ä¸­çš„ç™¾åˆ†ä½"""
        values = sorted([point['value'] for point in data])
        for i, v in enumerate(values):
            if v >= value:
                return (i / len(values)) * 100
        return 100.0

    def _is_suppressed(self, metric_name: str, timestamp: datetime) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¢«æŠ‘åˆ¶"""
        if metric_name not in self.suppression_rules:
            return False

        rule = self.suppression_rules[metric_name]

        # æ£€æŸ¥æ—¶é—´çª—å£å†…æ˜¯å¦æœ‰é‡å¤å‘Šè­¦
        recent_alerts = self.alert_history.get(metric_name, [])
        for alert in recent_alerts:
            alert_time = datetime.fromisoformat(alert['timestamp'])
            if timestamp - alert_time < timedelta(minutes=rule.get('suppression_minutes', 30)):
                return True

        return False
```

### æ™šä¸Šä»»åŠ¡ï¼šå‘Šè­¦é€šçŸ¥ç³»ç»Ÿ

#### 1. å¤šæ¸ é“é€šçŸ¥
```python
# backend/monitoring/notifications.py
import smtplib
import requests
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from typing import Dict, List
import json

class NotificationManager:
    def __init__(self, config: Dict):
        self.email_config = config.get('email', {})
        self.slack_config = config.get('slack', {})
        self.webhook_config = config.get('webhooks', {})
        self.sms_config = config.get('sms', {})

    async def send_alert(self, alert_data: Dict, channels: List[str]):
        """å‘é€å‘Šè­¦åˆ°å¤šä¸ªæ¸ é“"""
        tasks = []

        if 'email' in channels:
            tasks.append(self._send_email(alert_data))

        if 'slack' in channels:
            tasks.append(self._send_slack(alert_data))

        if 'webhook' in channels:
            tasks.append(self._send_webhook(alert_data))

        if 'sms' in channels and alert_data.get('severity') == 'critical':
            tasks.append(self._send_sms(alert_data))

        await asyncio.gather(*tasks, return_exceptions=True)

    async def _send_email(self, alert_data: Dict):
        """å‘é€é‚®ä»¶é€šçŸ¥"""
        try:
            msg = MIMEMultipart()
            msg['From'] = self.email_config['sender']
            msg['To'] = ', '.join(self.email_config['recipients'])
            msg['Subject'] = f"[{alert_data['severity'].upper()}] AI Hub å‘Šè­¦: {alert_data['metric_name']}"

            body = self._generate_email_body(alert_data)
            msg.attach(MIMEText(body, 'html'))

            server = smtplib.SMTP(self.email_config['smtp_host'], self.email_config['smtp_port'])
            server.starttls()
            server.login(self.email_config['username'], self.email_config['password'])
            server.send_message(msg)
            server.quit()

        except Exception as e:
            print(f"Failed to send email alert: {e}")

    async def _send_slack(self, alert_data: Dict):
        """å‘é€Slacké€šçŸ¥"""
        try:
            webhook_url = self.slack_config['webhook_url']

            payload = {
                "text": f"ğŸš¨ {alert_data['severity'].upper()} Alert",
                "attachments": [
                    {
                        "color": self._get_color_by_severity(alert_data['severity']),
                        "fields": [
                            {"title": "Metric", "value": alert_data['metric_name'], "short": True},
                            {"title": "Current Value", "value": str(alert_data['current_value']), "short": True},
                            {"title": "Severity", "value": alert_data['severity'], "short": True},
                            {"title": "Time", "value": alert_data['timestamp'], "short": True}
                        ],
                        "footer": "AI Hub Monitoring",
                        "ts": int(datetime.fromisoformat(alert_data['timestamp']).timestamp())
                    }
                ]
            }

            requests.post(webhook_url, json=payload, timeout=10)

        except Exception as e:
            print(f"Failed to send Slack alert: {e}")

    async def _send_webhook(self, alert_data: Dict):
        """å‘é€Webhooké€šçŸ¥"""
        try:
            for webhook in self.webhook_config.get('urls', []):
                payload = {
                    'alert': alert_data,
                    'timestamp': datetime.utcnow().isoformat(),
                    'service': 'ai-hub-monitoring'
                }

                requests.post(webhook['url'], json=payload,
                            headers=webhook.get('headers', {}),
                            timeout=10)

        except Exception as e:
            print(f"Failed to send webhook alert: {e}")

    async def _send_sms(self, alert_data: Dict):
        """å‘é€çŸ­ä¿¡é€šçŸ¥ï¼ˆä»…å…³é”®å‘Šè­¦ï¼‰"""
        try:
            # é›†æˆçŸ­ä¿¡æœåŠ¡æä¾›å•†API
            message = f"[CRITICAL] AI Hub: {alert_data['metric_name']} = {alert_data['current_value']}"

            for phone in self.sms_config.get('phone_numbers', []):
                # è°ƒç”¨çŸ­ä¿¡API
                pass

        except Exception as e:
            print(f"Failed to send SMS alert: {e}")

    def _generate_email_body(self, alert_data: Dict) -> str:
        """ç”Ÿæˆé‚®ä»¶å†…å®¹"""
        severity_colors = {
            'critical': '#ff4444',
            'warning': '#ffaa00',
            'info': '#00aaff'
        }

        color = severity_colors.get(alert_data['severity'], '#666666')

        return f"""
        <html>
        <body>
            <div style="font-family: Arial, sans-serif; max-width: 600px; margin: 0 auto;">
                <div style="background-color: {color}; color: white; padding: 20px; text-align: center;">
                    <h1>AI Hub ç³»ç»Ÿå‘Šè­¦</h1>
                    <p>ä¸¥é‡ç¨‹åº¦: {alert_data['severity'].upper()}</p>
                </div>

                <div style="padding: 20px; background-color: #f9f9f9;">
                    <h2>å‘Šè­¦è¯¦æƒ…</h2>
                    <table style="width: 100%; border-collapse: collapse;">
                        <tr>
                            <td style="padding: 8px; border: 1px solid #ddd;"><strong>æŒ‡æ ‡åç§°</strong></td>
                            <td style="padding: 8px; border: 1px solid #ddd;">{alert_data['metric_name']}</td>
                        </tr>
                        <tr>
                            <td style="padding: 8px; border: 1px solid #ddd;"><strong>å½“å‰å€¼</strong></td>
                            <td style="padding: 8px; border: 1px solid #ddd;">{alert_data['current_value']}</td>
                        </tr>
                        <tr>
                            <td style="padding: 8px; border: 1px solid #ddd;"><strong>å‘Šè­¦æ—¶é—´</strong></td>
                            <td style="padding: 8px; border: 1px solid #ddd;">{alert_data['timestamp']}</td>
                        </tr>
                    </table>

                    <div style="margin-top: 20px; padding: 15px; background-color: #e8f4f8; border-left: 4px solid {color};">
                        <p><strong>å»ºè®®æ“ä½œ:</strong></p>
                        <ul>
                            <li>ç™»å½•ç›‘æ§é¢æ¿æŸ¥çœ‹è¯¦ç»†æƒ…å†µ</li>
                            <li>æ£€æŸ¥ç›¸å…³æœåŠ¡çŠ¶æ€</li>
                            <li>å¦‚éœ€å¸®åŠ©ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒå›¢é˜Ÿ</li>
                        </ul>
                    </div>
                </div>

                <div style="background-color: #333; color: white; padding: 10px; text-align: center; font-size: 12px;">
                    <p>æ­¤é‚®ä»¶ç”± AI Hub ç›‘æ§ç³»ç»Ÿè‡ªåŠ¨å‘é€ï¼Œè¯·å‹¿å›å¤ã€‚</p>
                </div>
            </div>
        </body>
        </html>
        """

    def _get_color_by_severity(self, severity: str) -> str:
        """æ ¹æ®ä¸¥é‡ç¨‹åº¦è·å–é¢œè‰²"""
        colors = {
            'critical': '#ff4444',
            'warning': '#ffaa00',
            'info': '#00aaff'
        }
        return colors.get(severity, '#666666')
```

## Day 3 - æ€§èƒ½ä¼˜åŒ–å®æ–½

### ä¸Šåˆä»»åŠ¡ï¼šæ•°æ®åº“æ€§èƒ½ä¼˜åŒ–

#### 1. æŸ¥è¯¢ä¼˜åŒ–
```python
# backend/optimization/query_optimizer.py
import asyncio
import time
from typing import List, Dict, Any, Optional
from sqlalchemy import text, Index
from sqlalchemy.orm import Session
from contextlib import asynccontextmanager

class QueryOptimizer:
    def __init__(self, db_session: Session):
        self.db = db_session
        self.slow_queries = []
        self.query_cache = {}

    @asynccontextmanager
    async def profile_query(self, query_name: str):
        """æŸ¥è¯¢æ€§èƒ½åˆ†æ"""
        start_time = time.time()
        try:
            yield
        finally:
            duration = time.time() - start_time
            if duration > 1.0:  # è¶…è¿‡1ç§’çš„æ…¢æŸ¥è¯¢
                self.slow_queries.append({
                    'query_name': query_name,
                    'duration': duration,
                    'timestamp': time.time()
                })

    async def create_optimal_indexes(self):
        """åˆ›å»ºä¼˜åŒ–ç´¢å¼•"""
        indexes = [
            # ç”¨æˆ·è¡¨ç´¢å¼•
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_email_active ON users(email) WHERE is_active = true;",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_org_created ON users(organization_id, created_at);",

            # APIå¯†é’¥è¡¨ç´¢å¼•
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_api_keys_user_active ON api_keys(user_id, is_active);",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_api_keys_last_used ON api_keys(last_used_at DESC);",

            # ä¼šè¯è¡¨ç´¢å¼•
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_sessions_user_created ON sessions(user_id, created_at DESC);",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_sessions_active ON sessions(is_active) WHERE is_active = true;",

            # ä½¿ç”¨è®°å½•è¡¨ç´¢å¼•
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_usage_records_user_date ON usage_records(user_id, created_at);",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_usage_records_model_date ON usage_records(model_name, created_at);",

            # ç›‘æ§æ•°æ®ç´¢å¼•
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_performance_metrics_name_time ON performance_metrics(metric_name, timestamp DESC);",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_business_metrics_user_time ON business_metrics(user_id, timestamp DESC);"
        ]

        for index_sql in indexes:
            try:
                await self.db.execute(text(index_sql))
                await self.db.commit()
                print(f"Created index: {index_sql.split('idx_')[1].split(' ')[0]}")
            except Exception as e:
                print(f"Failed to create index: {e}")
                await self.db.rollback()

    async def analyze_slow_queries(self):
        """åˆ†ææ…¢æŸ¥è¯¢"""
        # è·å–æ…¢æŸ¥è¯¢ç»Ÿè®¡
        slow_query_sql = """
        SELECT query, mean_time, calls, total_time
        FROM pg_stat_statements
        WHERE mean_time > 1000  -- è¶…è¿‡1ç§’çš„æŸ¥è¯¢
        ORDER BY mean_time DESC
        LIMIT 10;
        """

        result = await self.db.execute(text(slow_query_sql))
        slow_queries = result.fetchall()

        analysis = []
        for query in slow_queries:
            analysis.append({
                'query': query.query[:200] + '...' if len(query.query) > 200 else query.query,
                'mean_time': query.mean_time,
                'calls': query.calls,
                'total_time': query.total_time,
                'optimization_suggestions': self._suggest_optimizations(query.query)
            })

        return analysis

    def _suggest_optimizations(self, query: str) -> List[str]:
        """ä¸ºæŸ¥è¯¢æä¾›ä¼˜åŒ–å»ºè®®"""
        suggestions = []

        query_lower = query.lower()

        # æ£€æŸ¥æ˜¯å¦ç¼ºå°‘WHEREæ¡ä»¶
        if 'where' not in query_lower and 'delete' not in query_lower:
            suggestions.append("è€ƒè™‘æ·»åŠ WHEREæ¡ä»¶é™åˆ¶ç»“æœé›†")

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨SELECT *
        if 'select *' in query_lower:
            suggestions.append("é¿å…ä½¿ç”¨SELECT *ï¼ŒåªæŸ¥è¯¢éœ€è¦çš„åˆ—")

        # æ£€æŸ¥æ˜¯å¦æœ‰é€‚å½“çš„LIMIT
        if 'limit' not in query_lower and 'count(' not in query_lower:
            suggestions.append("è€ƒè™‘æ·»åŠ LIMITé™åˆ¶è¿”å›çš„è¡Œæ•°")

        # æ£€æŸ¥JOINä¼˜åŒ–
        if 'join' in query_lower and 'hash join' not in query_lower:
            suggestions.append("è€ƒè™‘ä½¿ç”¨HASH JOINæˆ–ä¼˜åŒ–JOINæ¡ä»¶")

        return suggestions

    async def optimize_table_statistics(self):
        """æ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯"""
        tables = [
            'users', 'organizations', 'api_keys', 'sessions',
            'usage_records', 'performance_metrics', 'business_metrics'
        ]

        for table in tables:
            try:
                await self.db.execute(text(f"ANALYZE {table};"))
                print(f"Updated statistics for table: {table}")
            except Exception as e:
                print(f"Failed to analyze table {table}: {e}")

        await self.db.commit()
```

#### 2. è¿æ¥æ± ä¼˜åŒ–
```python
# backend/optimization/connection_pool.py
import asyncio
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from typing import Dict, Any

class ConnectionPoolManager:
    def __init__(self, database_url: str):
        self.database_url = database_url
        self.engine = None
        self.session_factory = None
        self.pool_stats = {
            'total_connections': 0,
            'active_connections': 0,
            'idle_connections': 0,
            'overflow_connections': 0
        }

    async def initialize_pool(self, pool_config: Dict[str, Any] = None):
        """åˆå§‹åŒ–è¿æ¥æ± """
        default_config = {
            'pool_size': 20,  # åŸºç¡€è¿æ¥æ•°
            'max_overflow': 30,  # æº¢å‡ºè¿æ¥æ•°
            'pool_timeout': 30,  # è·å–è¿æ¥è¶…æ—¶æ—¶é—´
            'pool_recycle': 3600,  # è¿æ¥å›æ”¶æ—¶é—´ï¼ˆç§’ï¼‰
            'pool_pre_ping': True,  # è¿æ¥å‰pingæ£€æŸ¥
            'echo': False,  # SQLæ—¥å¿—
        }

        if pool_config:
            default_config.update(pool_config)

        self.engine = create_async_engine(
            self.database_url,
            **default_config
        )

        self.session_factory = sessionmaker(
            self.engine,
            class_=AsyncSession,
            expire_on_commit=False
        )

        print("Database connection pool initialized")
        await self._monitor_pool()

    async def get_session(self) -> AsyncSession:
        """è·å–æ•°æ®åº“ä¼šè¯"""
        if not self.session_factory:
            raise RuntimeError("Connection pool not initialized")

        session = self.session_factory()
        await self._update_pool_stats()
        return session

    async def _update_pool_stats(self):
        """æ›´æ–°è¿æ¥æ± ç»Ÿè®¡ä¿¡æ¯"""
        if self.engine:
            pool = self.engine.pool
            self.pool_stats = {
                'total_connections': pool.size() + pool.overflow(),
                'active_connections': pool.checkedin(),
                'idle_connections': pool.checkedout(),
                'overflow_connections': pool.overflow()
            }

    async def _monitor_pool(self):
        """ç›‘æ§è¿æ¥æ± çŠ¶æ€"""
        while True:
            await self._update_pool_stats()

            # æ£€æŸ¥è¿æ¥æ± å¥åº·çŠ¶æ€
            if self.pool_stats['overflow_connections'] > 0:
                print(f"Warning: {self.pool_stats['overflow_connections']} overflow connections in use")

            if self.pool_stats['active_connections'] > self.pool_stats['total_connections'] * 0.8:
                print("Warning: High connection pool usage detected")

            await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

    async def close_pool(self):
        """å…³é—­è¿æ¥æ± """
        if self.engine:
            await self.engine.dispose()
            print("Database connection pool closed")

    def get_pool_stats(self) -> Dict[str, int]:
        """è·å–è¿æ¥æ± ç»Ÿè®¡ä¿¡æ¯"""
        return self.pool_stats.copy()
```

### ä¸‹åˆä»»åŠ¡ï¼šç¼“å­˜ç³»ç»Ÿä¼˜åŒ–

#### 1. Rediså¤šçº§ç¼“å­˜
```python
# backend/optimization/cache_manager.py
import json
import pickle
import hashlib
from typing import Any, Optional, Dict, List
from datetime import datetime, timedelta
import redis.asyncio as redis
from functools import wraps

class CacheManager:
    def __init__(self, redis_url: str):
        self.redis_client = None
        self.redis_url = redis_url
        self.local_cache = {}  # æœ¬åœ°ç¼“å­˜
        self.local_cache_ttl = {}  # æœ¬åœ°ç¼“å­˜è¿‡æœŸæ—¶é—´
        self.cache_stats = {
            'hits': 0,
            'misses': 0,
            'local_hits': 0,
            'redis_hits': 0,
            'sets': 0
        }

    async def initialize(self):
        """åˆå§‹åŒ–Redisè¿æ¥"""
        self.redis_client = redis.from_url(self.redis_url, decode_responses=False)
        await self.redis_client.ping()
        print("Redis cache initialized")

    def cache_key(self, prefix: str, *args, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = {
            'args': args,
            'kwargs': sorted(kwargs.items())
        }
        key_hash = hashlib.md5(json.dumps(key_data, sort_keys=True).encode()).hexdigest()
        return f"{prefix}:{key_hash}"

    async def get(self, key: str, use_local_cache: bool = True) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼"""
        # æ£€æŸ¥æœ¬åœ°ç¼“å­˜
        if use_local_cache and key in self.local_cache:
            if key in self.local_cache_ttl and datetime.now() < self.local_cache_ttl[key]:
                self.cache_stats['hits'] += 1
                self.cache_stats['local_hits'] += 1
                return self.local_cache[key]
            else:
                # æœ¬åœ°ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
                del self.local_cache[key]
                if key in self.local_cache_ttl:
                    del self.local_cache_ttl[key]

        # æ£€æŸ¥Redisç¼“å­˜
        if self.redis_client:
            try:
                cached_data = await self.redis_client.get(key)
                if cached_data:
                    # ååºåˆ—åŒ–æ•°æ®
                    data = pickle.loads(cached_data)

                    # æ›´æ–°æœ¬åœ°ç¼“å­˜
                    if use_local_cache:
                        self.local_cache[key] = data
                        self.local_cache_ttl[key] = datetime.now() + timedelta(minutes=5)

                    self.cache_stats['hits'] += 1
                    self.cache_stats['redis_hits'] += 1
                    return data
            except Exception as e:
                print(f"Redis cache get error: {e}")

        self.cache_stats['misses'] += 1
        return None

    async def set(self, key: str, value: Any, ttl: int = 3600,
                  use_local_cache: bool = True, local_ttl_minutes: int = 5):
        """è®¾ç½®ç¼“å­˜å€¼"""
        try:
            # åºåˆ—åŒ–æ•°æ®
            serialized_data = pickle.dumps(value)

            # è®¾ç½®Redisç¼“å­˜
            if self.redis_client:
                await self.redis_client.setex(key, ttl, serialized_data)

            # è®¾ç½®æœ¬åœ°ç¼“å­˜
            if use_local_cache:
                self.local_cache[key] = value
                self.local_cache_ttl[key] = datetime.now() + timedelta(minutes=local_ttl_minutes)

            self.cache_stats['sets'] += 1

        except Exception as e:
            print(f"Cache set error: {e}")

    async def delete(self, key: str):
        """åˆ é™¤ç¼“å­˜"""
        # åˆ é™¤æœ¬åœ°ç¼“å­˜
        if key in self.local_cache:
            del self.local_cache[key]
        if key in self.local_cache_ttl:
            del self.local_cache_ttl[key]

        # åˆ é™¤Redisç¼“å­˜
        if self.redis_client:
            await self.redis_client.delete(key)

    async def clear_pattern(self, pattern: str):
        """æ¸…é™¤åŒ¹é…æ¨¡å¼çš„ç¼“å­˜"""
        # æ¸…é™¤æœ¬åœ°ç¼“å­˜
        keys_to_delete = [k for k in self.local_cache.keys() if pattern in k]
        for key in keys_to_delete:
            del self.local_cache[key]
            if key in self.local_cache_ttl:
                del self.local_cache_ttl[key]

        # æ¸…é™¤Redisç¼“å­˜
        if self.redis_client:
            keys = await self.redis_client.keys(f"*{pattern}*")
            if keys:
                await self.redis_client.delete(*keys)

    def cached(self, prefix: str, ttl: int = 3600):
        """ç¼“å­˜è£…é¥°å™¨"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # ç”Ÿæˆç¼“å­˜é”®
                cache_key = self.cache_key(prefix, *args, **kwargs)

                # å°è¯•ä»ç¼“å­˜è·å–
                cached_result = await self.get(cache_key)
                if cached_result is not None:
                    return cached_result

                # æ‰§è¡Œå‡½æ•°
                result = await func(*args, **kwargs)

                # ç¼“å­˜ç»“æœ
                await self.set(cache_key, result, ttl)

                return result
            return wrapper
        return decorator

    async def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self.cache_stats['hits'] + self.cache_stats['misses']
        hit_rate = (self.cache_stats['hits'] / total_requests * 100) if total_requests > 0 else 0

        stats = {
            **self.cache_stats,
            'hit_rate_percent': round(hit_rate, 2),
            'local_cache_size': len(self.local_cache),
            'local_cache_hit_rate': (self.cache_stats['local_hits'] / self.cache_stats['hits'] * 100) if self.cache_stats['hits'] > 0 else 0
        }

        # è·å–Redisä¿¡æ¯
        if self.redis_client:
            try:
                redis_info = await self.redis_client.info()
                stats.update({
                    'redis_used_memory': redis_info.get('used_memory_human'),
                    'redis_connected_clients': redis_info.get('connected_clients'),
                    'redis_keyspace_hits': redis_info.get('keyspace_hits', 0),
                    'redis_keyspace_misses': redis_info.get('keyspace_misses', 0)
                })
            except Exception as e:
                print(f"Failed to get Redis info: {e}")

        return stats

    async def cleanup_expired_local_cache(self):
        """æ¸…ç†è¿‡æœŸçš„æœ¬åœ°ç¼“å­˜"""
        current_time = datetime.now()
        expired_keys = [
            key for key, expiry_time in self.local_cache_ttl.items()
            if current_time >= expiry_time
        ]

        for key in expired_keys:
            if key in self.local_cache:
                del self.local_cache[key]
            del self.local_cache_ttl[key]

        if expired_keys:
            print(f"Cleaned up {len(expired_keys)} expired local cache entries")

# å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
cache_manager = CacheManager("redis://localhost:6379/0")
```

#### 2. åº”ç”¨å±‚ç¼“å­˜ç­–ç•¥
```python
# backend/optimization/api_cache.py
from fastapi import Request, Response
from fastapi.responses import JSONResponse
from typing import Optional, Dict, Any
import hashlib
import json

class APICacheMiddleware:
    def __init__(self, cache_manager: CacheManager):
        self.cache_manager = cache_manager
        # å¯ç¼“å­˜çš„ç«¯ç‚¹é…ç½®
        self.cacheable_endpoints = {
            '/api/v1/models': {'ttl': 300, 'vary_by_user': False},
            '/api/v1/stats': {'ttl': 60, 'vary_by_user': True},
            '/api/v1/config': {'ttl': 600, 'vary_by_user': True},
            '/api/v1/health': {'ttl': 30, 'vary_by_user': False},
        }

    async def __call__(self, request: Request, call_next):
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¯ç¼“å­˜ç«¯ç‚¹
        if request.method != 'GET' or request.url.path not in self.cacheable_endpoints:
            return await call_next(request)

        cache_config = self.cacheable_endpoints[request.url.path]

        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = await self._generate_cache_key(request, cache_config)

        # å°è¯•ä»ç¼“å­˜è·å–
        cached_response = await self.cache_manager.get(cache_key)
        if cached_response:
            return JSONResponse(
                content=cached_response['content'],
                status_code=cached_response['status_code'],
                headers=cached_response['headers']
            )

        # æ‰§è¡Œè¯·æ±‚
        response = await call_next(request)

        # ç¼“å­˜å“åº”
        if response.status_code == 200:
            response_body = b''
            async for chunk in response.body_iterator:
                response_body += chunk

            try:
                content = json.loads(response_body.decode())
                await self.cache_manager.set(
                    cache_key,
                    {
                        'content': content,
                        'status_code': response.status_code,
                        'headers': dict(response.headers)
                    },
                    ttl=cache_config['ttl']
                )
            except json.JSONDecodeError:
                pass  # ä¸ç¼“å­˜éJSONå“åº”

        return response

    async def _generate_cache_key(self, request: Request, cache_config: Dict) -> str:
        """ç”ŸæˆAPIç¼“å­˜é”®"""
        key_parts = [
            f"api_cache:{request.url.path}",
            f"method:{request.method}"
        ]

        # å¦‚æœéœ€è¦æ ¹æ®ç”¨æˆ·å˜åŒ–
        if cache_config.get('vary_by_user', False):
            user_id = getattr(request.state, 'user_id', 'anonymous')
            key_parts.append(f"user:{user_id}")

        # æ·»åŠ æŸ¥è¯¢å‚æ•°
        if request.query_params:
            sorted_params = sorted(request.query_params.items())
            key_parts.append(f"params:{json.dumps(sorted_params)}")

        return ":".join(key_parts)

# æ™ºèƒ½ç¼“å­˜é¢„çƒ­
class CacheWarmup:
    def __init__(self, cache_manager: CacheManager):
        self.cache_manager = cache_manager

    async def warmup_cache(self):
        """é¢„çƒ­ç¼“å­˜"""
        warmup_tasks = [
            self._warmup_models_cache(),
            self._warmup_config_cache(),
            self._warmup_stats_cache()
        ]

        await asyncio.gather(*warmup_tasks, return_exceptions=True)
        print("Cache warmup completed")

    async def _warmup_models_cache(self):
        """é¢„çƒ­æ¨¡å‹ç¼“å­˜"""
        try:
            # è¿™é‡Œéœ€è¦è°ƒç”¨å®é™…çš„æ¨¡å‹API
            models_data = await self._fetch_models_data()
            await self.cache_manager.set('api_cache:/api/v1/models:method:GET', models_data, ttl=300)
        except Exception as e:
            print(f"Failed to warmup models cache: {e}")

    async def _warmup_config_cache(self):
        """é¢„çƒ­é…ç½®ç¼“å­˜"""
        try:
            config_data = await self._fetch_config_data()
            await self.cache_manager.set('api_cache:/api/v1/config:method:GET', config_data, ttl=600)
        except Exception as e:
            print(f"Failed to warmup config cache: {e}")

    async def _warmup_stats_cache(self):
        """é¢„çƒ­ç»Ÿè®¡æ•°æ®ç¼“å­˜"""
        try:
            stats_data = await self._fetch_stats_data()
            await self.cache_manager.set('api_cache:/api/v1/stats:method:GET', stats_data, ttl=60)
        except Exception as e:
            print(f"Failed to warmup stats cache: {e}")

    async def _fetch_models_data(self):
        """è·å–æ¨¡å‹æ•°æ®ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        # å®é™…å®ç°ä¸­è°ƒç”¨æ¨¡å‹æœåŠ¡
        return {"models": []}

    async def _fetch_config_data(self):
        """è·å–é…ç½®æ•°æ®ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        # å®é™…å®ç°ä¸­è°ƒç”¨é…ç½®æœåŠ¡
        return {"config": {}}

    async def _fetch_stats_data(self):
        """è·å–ç»Ÿè®¡æ•°æ®ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        # å®é™…å®ç°ä¸­è°ƒç”¨ç»Ÿè®¡æœåŠ¡
        return {"stats": {}}
```

### æ™šä¸Šä»»åŠ¡ï¼šAPIæ€§èƒ½ä¼˜åŒ–

#### 1. å¼‚æ­¥ä¼˜åŒ–
```python
# backend/optimization/async_optimizer.py
import asyncio
import time
from typing import List, Dict, Any, Callable, Awaitable
from concurrent.futures import ThreadPoolExecutor
import functools

class AsyncOptimizer:
    def __init__(self):
        self.thread_pool = ThreadPoolExecutor(max_workers=10)
        self.performance_stats = {}

    def batch_async_requests(self, coroutines: List[Awaitable],
                           max_concurrency: int = 10) -> List[Any]:
        """æ‰¹é‡æ‰§è¡Œå¼‚æ­¥è¯·æ±‚"""
        semaphore = asyncio.Semaphore(max_concurrency)

        async def bounded_coroutine(coro):
            async with semaphore:
                return await coro

        bounded_coroutines = [bounded_coroutine(coro) for coro in coroutines]
        return asyncio.run(bounded_coroutines)

    def run_in_thread(self, func: Callable, *args, **kwargs):
        """åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥å‡½æ•°"""
        loop = asyncio.get_event_loop()
        return loop.run_in_executor(self.thread_pool, functools.partial(func, *args, **kwargs))

    def async_timed(self, operation_name: str):
        """å¼‚æ­¥è®¡æ—¶è£…é¥°å™¨"""
        def decorator(func):
            @functools.wraps(func)
            async def wrapper(*args, **kwargs):
                start_time = time.time()
                try:
                    result = await func(*args, **kwargs)
                    duration = time.time() - start_time

                    # è®°å½•æ€§èƒ½ç»Ÿè®¡
                    if operation_name not in self.performance_stats:
                        self.performance_stats[operation_name] = []
                    self.performance_stats[operation_name].append({
                        'duration': duration,
                        'timestamp': time.time(),
                        'success': True
                    })

                    return result
                except Exception as e:
                    duration = time.time() - start_time

                    if operation_name not in self.performance_stats:
                        self.performance_stats[operation_name] = []
                    self.performance_stats[operation_name].append({
                        'duration': duration,
                        'timestamp': time.time(),
                        'success': False,
                        'error': str(e)
                    })

                    raise
            return wrapper
        return decorator

    async def parallel_api_calls(self, api_calls: List[Dict[str, Any]]) -> List[Any]:
        """å¹¶è¡Œæ‰§è¡Œå¤šä¸ªAPIè°ƒç”¨"""
        async def execute_api_call(call_config):
            api_func = call_config['function']
            args = call_config.get('args', [])
            kwargs = call_config.get('kwargs', {})

            try:
                result = await api_func(*args, **kwargs)
                return {
                    'success': True,
                    'result': result,
                    'call_id': call_config.get('id')
                }
            except Exception as e:
                return {
                    'success': False,
                    'error': str(e),
                    'call_id': call_config.get('id')
                }

        tasks = [execute_api_call(call) for call in api_calls]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        return results

    def get_performance_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = {}

        for operation, stats in self.performance_stats.items():
            if not stats:
                continue

            successful_calls = [s for s in stats if s['success']]
            failed_calls = [s for s in stats if not s['success']]

            if successful_calls:
                durations = [s['duration'] for s in successful_calls]
                report[operation] = {
                    'total_calls': len(stats),
                    'successful_calls': len(successful_calls),
                    'failed_calls': len(failed_calls),
                    'success_rate': len(successful_calls) / len(stats) * 100,
                    'avg_duration': sum(durations) / len(durations),
                    'min_duration': min(durations),
                    'max_duration': max(durations),
                    'p95_duration': sorted(durations)[int(len(durations) * 0.95)],
                    'recent_errors': [call['error'] for call in failed_calls[-5:]]
                }

        return report

# ä½¿ç”¨ç¤ºä¾‹
async_optimizer = AsyncOptimizer()

@async_optimizer.async_timed('ai_model_call')
async def optimized_ai_call(prompt: str, model: str):
    """ä¼˜åŒ–çš„AIæ¨¡å‹è°ƒç”¨"""
    # å®ç°ä¼˜åŒ–çš„AIè°ƒç”¨é€»è¾‘
    pass
```

#### 2. å“åº”å‹ç¼©
```python
# backend/optimization/compression.py
import gzip
import json
from fastapi import Response, Request
from fastapi.responses import JSONResponse
from typing import Any, Dict

class ResponseCompressor:
    def __init__(self, min_size: int = 1024):  # 1KBä»¥ä¸Šå‹ç¼©
        self.min_size = min_size

    def compress_response(self, response_data: Any, accept_encoding: str) -> Response:
        """å‹ç¼©å“åº”æ•°æ®"""
        # åºåˆ—åŒ–æ•°æ®
        if isinstance(response_data, dict):
            content = json.dumps(response_data).encode('utf-8')
        else:
            content = str(response_data).encode('utf-8')

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
        if len(content) < self.min_size or 'gzip' not in accept_encoding.lower():
            return JSONResponse(content=response_data)

        # å‹ç¼©æ•°æ®
        compressed_content = gzip.compress(content)

        # è®¡ç®—å‹ç¼©ç‡
        compression_ratio = (1 - len(compressed_content) / len(content)) * 100

        headers = {
            'Content-Encoding': 'gzip',
            'Content-Type': 'application/json',
            'X-Compression-Ratio': f"{compression_ratio:.2f}%"
        }

        return Response(
            content=compressed_content,
            headers=headers,
            media_type="application/json"
        )

class StreamingOptimizer:
    def __init__(self, chunk_size: int = 8192):
        self.chunk_size = chunk_size

    async def stream_with_backpressure(self, generator, max_queue_size: int = 100):
        """å¸¦èƒŒå‹æ§åˆ¶çš„æµå¼å“åº”"""
        queue = asyncio.Queue(maxsize=max_queue_size)

        async def producer():
            try:
                async for item in generator:
                    await queue.put(item)
                await queue.put(None)  # ç»“æŸæ ‡è®°
            except Exception as e:
                await queue.put({'error': str(e)})

        async def consumer():
            producer_task = asyncio.create_task(producer())

            try:
                while True:
                    item = await queue.get()
                    if item is None:
                        break
                    if isinstance(item, dict) and 'error' in item:
                        raise Exception(item['error'])
                    yield item
                    queue.task_done()
            finally:
                producer_task.cancel()
                try:
                    await producer_task
                except asyncio.CancelledError:
                    pass

        return consumer()

# å“åº”ä¼˜åŒ–ä¸­é—´ä»¶
class OptimizationMiddleware:
    def __init__(self):
        self.compressor = ResponseCompressor()
        self.streaming_optimizer = StreamingOptimizer()

    async def __call__(self, request: Request, call_next):
        start_time = time.time()

        response = await call_next(request)

        # æ·»åŠ æ€§èƒ½å¤´
        process_time = time.time() - start_time
        response.headers["X-Process-Time"] = str(process_time)

        # å‹ç¼©å“åº”ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
        if (request.headers.get("accept-encoding", "").lower().find("gzip") != -1 and
            isinstance(response, JSONResponse) and
            len(str(response.body)) > self.compressor.min_size):

            # é‡æ–°æ„å»ºå‹ç¼©å“åº”
            return self.compressor.compress_response(
                response.body,
                request.headers.get("accept-encoding", "")
            )

        return response
```

## æ€»ç»“

Week 8å°†å»ºç«‹å®Œæ•´çš„æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–ä½“ç³»ï¼š

### æ ¸å¿ƒåŠŸèƒ½
1. **å®æ—¶ç›‘æ§ä»ªè¡¨æ¿** - ç³»ç»ŸæŒ‡æ ‡ã€ä¸šåŠ¡æŒ‡æ ‡å¯è§†åŒ–
2. **æ™ºèƒ½å‘Šè­¦ç³»ç»Ÿ** - åŸºäºæœºå™¨å­¦ä¹ çš„å¼‚å¸¸æ£€æµ‹
3. **å¤šçº§ç¼“å­˜ä¼˜åŒ–** - Redis + æœ¬åœ°ç¼“å­˜ç­–ç•¥
4. **æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–** - æŸ¥è¯¢ä¼˜åŒ–ã€è¿æ¥æ± ç®¡ç†
5. **APIæ€§èƒ½ä¼˜åŒ–** - å¼‚æ­¥å¤„ç†ã€å“åº”å‹ç¼©

### é¢„æœŸæˆæœ
- **ç³»ç»Ÿå“åº”æ—¶é—´** < 100ms (P95)
- **ç›‘æ§è¦†ç›–ç‡** 100%
- **å‘Šè­¦å‡†ç¡®ç‡** > 95%
- **ç¼“å­˜å‘½ä¸­ç‡** > 80%
- **ç³»ç»Ÿå¯ç”¨æ€§** > 99.9%

è¿™äº›åŠŸèƒ½å°†ç¡®ä¿AI Hubå¹³å°åœ¨é«˜å¹¶å‘åœºæ™¯ä¸‹çš„ç¨³å®šæ€§å’Œé«˜æ€§èƒ½è¡¨ç°ã€‚